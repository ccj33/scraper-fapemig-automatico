#!/usr/bin/env python3
"""
Scraper Final Melhorado - Sistema Completo de Extra√ß√£o
======================================================

Vers√£o 3.0 - Extra√ß√£o inteligente, PDFs e relat√≥rios leg√≠veis
"""

import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import os
import time
import json
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
import chromedriver_autoinstaller
import logging
import re
from typing import List, Dict, Optional
import random

# Importar o gerador de resumos
from gerador_resumo_melhorado import GeradorResumoMelhorado

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper_final.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ScraperFinalMelhorado:
    """Scraper final com todas as melhorias"""
    
    def __init__(self):
        self.driver = None
        self.results = {
            'ufmg': [],
            'fapemig': [],
            'cnpq': [],
            'timestamp': datetime.now().isoformat(),
            'total_editais': 0
        }
        
    def setup_driver(self):
        """Configura o driver do Chrome"""
        try:
            chromedriver_autoinstaller.install()
            
            options = Options()
            options.add_argument("--headless")
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("--disable-gpu")
            options.add_argument("--window-size=1920,1080")
            options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
            
            self.driver = webdriver.Chrome(options=options)
            logger.info("‚úÖ Driver Chrome configurado com sucesso")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao configurar driver: {e}")
            raise
            
    def run_scraping(self):
        """Executa o scraping de todos os sites"""
        try:
            logger.info("üöÄ Iniciando scraping final melhorado...")
            
            # UFMG
            try:
                self.results['ufmg'] = self._scrape_ufmg()
                logger.info(f"‚úÖ UFMG: {len(self.results['ufmg'])} editais extra√≠dos")
            except Exception as e:
                logger.error(f"‚ùå Erro UFMG: {e}")
                self.results['ufmg'] = []
                
            # FAPEMIG
            try:
                self.results['fapemig'] = self._scrape_fapemig()
                logger.info(f"‚úÖ FAPEMIG: {len(self.results['fapemig'])} chamadas extra√≠das")
            except Exception as e:
                logger.error(f"‚ùå Erro FAPEMIG: {e}")
                self.results['fapemig'] = []
                
            # CNPq
            try:
                self.results['cnpq'] = self._scrape_cnpq()
                logger.info(f"‚úÖ CNPq: {len(self.results['cnpq'])} chamadas extra√≠das")
            except Exception as e:
                logger.error(f"‚ùå Erro CNPq: {e}")
                self.results['cnpq'] = []
                
            # Calcular total
            self.results['total_editais'] = (
                len(self.results['ufmg']) + 
                len(self.results['fapemig']) + 
                len(self.results['cnpq'])
            )
            
            logger.info(f"üéâ Scraping conclu√≠do! Total: {self.results['total_editais']} oportunidades")
            
        except Exception as e:
            logger.error(f"‚ùå Erro geral no scraping: {e}")
            
    def _scrape_ufmg(self) -> List[Dict]:
        """Scraper espec√≠fico para UFMG"""
        logger.info("üöÄ Iniciando extra√ß√£o UFMG...")
        
        try:
            self.driver.get("https://www.ufmg.br/prograd/editais-chamadas/")
            time.sleep(3)
            
            editais = []
            
            # Procurar por links de editais
            links = self.driver.find_elements(By.CSS_SELECTOR, 'a[href*=".pdf"], a[href*="edital"]')
            
            for link in links[:10]:  # Limitar a 10 para n√£o sobrecarregar
                try:
                    titulo = link.text.strip()
                    href = link.get_attribute("href")
                    
                    if titulo and href and ("edital" in titulo.lower() or "pdf" in href.lower()):
                        edital = {
                            'titulo': titulo,
                            'url': href,
                            'fonte': 'UFMG',
                            'data_extracao': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        }
                        
                        # Tentar extrair data pr√≥xima
                        try:
                            parent = link.find_element(By.XPATH, "./..")
                            parent_text = parent.text
                            date_match = re.search(r'\d{2}/\d{2}/\d{4}', parent_text)
                            if date_match:
                                edital['data'] = date_match.group()
                        except:
                            edital['data'] = "Data n√£o encontrada"
                            
                        editais.append(edital)
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao processar link UFMG: {e}")
                    continue
                    
            return editais
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair UFMG: {e}")
            return []
            
    def _scrape_fapemig(self) -> List[Dict]:
        """Scraper espec√≠fico para FAPEMIG"""
        logger.info("üöÄ Iniciando extra√ß√£o FAPEMIG...")
        
        try:
            self.driver.get("http://www.fapemig.br/pt/chamadas_abertas_oportunidades_fapemig/")
            time.sleep(3)
            
            chamadas = []
            
            # Procurar por t√≠tulos de chamadas
            titulos = self.driver.find_elements(By.CSS_SELECTOR, 'h1, h2, h3, h4, h5, h6')
            
            for titulo in titulos[:8]:  # Limitar a 8
                try:
                    texto = titulo.text.strip()
                    
                    if texto and any(palavra in texto.lower() for palavra in ['chamada', 'edital', 'oportunidade']):
                        chamada = {
                            'titulo': texto,
                            'fonte': 'FAPEMIG',
                            'data_extracao': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        }
                        
                        # Procurar link pr√≥ximo
                        try:
                            parent = titulo.find_element(By.XPATH, "./..")
                            links = parent.find_elements(By.TAG_NAME, "a")
                            for link in links:
                                href = link.get_attribute("href")
                                if href and href.startswith("http"):
                                    chamada['url'] = href
                                    break
                        except:
                            chamada['url'] = "Link n√£o encontrado"
                            
                        chamadas.append(chamada)
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao processar t√≠tulo FAPEMIG: {e}")
                    continue
                    
            return chamadas
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair FAPEMIG: {e}")
            return []
            
    def _scrape_cnpq(self) -> List[Dict]:
        """Scraper espec√≠fico para CNPq"""
        logger.info("üöÄ Iniciando extra√ß√£o CNPq...")
        
        try:
            self.driver.get("http://memoria2.cnpq.br/web/guest/chamadas-publicas")
            time.sleep(3)
            
            chamadas = []
            
            # Procurar por t√≠tulos de chamadas
            titulos = self.driver.find_elements(By.XPATH, '//h4[contains(text(), "CHAMADA") or contains(text(), "Chamada")]')
            
            for titulo in titulos[:6]:  # Limitar a 6
                try:
                    texto = titulo.text.strip()
                    
                    if texto:
                        chamada = {
                            'titulo': texto,
                            'fonte': 'CNPq',
                            'data_extracao': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        }
                        
                        # Procurar per√≠odo pr√≥ximo
                        try:
                            parent = titulo.find_element(By.XPATH, "./..")
                            parent_text = parent.text
                            periodo_match = re.search(r'\d{2}/\d{2}/\d{4}\s*a\s*\d{2}/\d{2}/\d{4}', parent_text)
                            if periodo_match:
                                chamada['periodo_inscricao'] = periodo_match.group()
                        except:
                            chamada['periodo_inscricao'] = "Per√≠odo n√£o encontrado"
                            
                        chamadas.append(chamada)
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao processar t√≠tulo CNPq: {e}")
                    continue
                    
            return chamadas
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair CNPq: {e}")
            return []
            
    def save_results(self):
        """Salva os resultados em arquivos"""
        try:
            # Salvar JSON completo
            with open("resultados_finais.json", "w", encoding="utf-8") as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
                
            # Gerar resumo melhorado
            gerador = GeradorResumoMelhorado(self.results)
            resumo = gerador.gerar_resumo_completo()
            
            # Salvar resumo em texto
            with open("relatorio_final.txt", "w", encoding="utf-8") as f:
                f.write(resumo)
                
            logger.info("‚úÖ Resultados salvos com sucesso")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar resultados: {e}")
            
    def cleanup(self):
        """Limpa recursos"""
        if self.driver:
            try:
                self.driver.quit()
                logger.info("‚úÖ Driver fechado com sucesso")
            except:
                pass

def send_email_final(msg_text: str, subject: str = "üöÄ RELAT√ìRIO FINAL - Oportunidades Encontradas!"):
    """Envia email com o relat√≥rio final"""
    try:
        msg = MIMEMultipart()
        msg['Subject'] = subject
        msg['From'] = os.environ['EMAIL_USER']
        msg['To'] = os.environ['EMAIL_USER']
        
        text_part = MIMEText(msg_text, 'plain', 'utf-8')
        msg.attach(text_part)
        
        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:
            server.login(os.environ['EMAIL_USER'], os.environ['EMAIL_PASS'])
            server.send_message(msg)
            
        logger.info("‚úÖ Email enviado com sucesso!")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao enviar email: {e}")
        return False

def main():
    """Fun√ß√£o principal"""
    logger.info("=" * 60)
    logger.info("üöÄ SCRAPER FINAL MELHORADO - EDITAIS E CHAMADAS")
    logger.info("=" * 60)
    
    scraper = ScraperFinalMelhorado()
    
    try:
        # Configurar driver
        scraper.setup_driver()
        
        # Executar scraping
        scraper.run_scraping()
        
        # Salvar resultados
        scraper.save_results()
        
        # Preparar email
        if scraper.results['total_editais'] > 0:
            # Ler o relat√≥rio gerado
            with open("relatorio_final.txt", "r", encoding="utf-8") as f:
                email_content = f.read()
            
            # Enviar email se as vari√°veis estiverem configuradas
            if 'EMAIL_USER' in os.environ and 'EMAIL_PASS' in os.environ:
                logger.info("üìß Enviando email...")
                if send_email_final(email_content):
                    logger.info("‚úÖ Processo conclu√≠do com sucesso!")
                else:
                    logger.warning("‚ö†Ô∏è Scraping conclu√≠do, mas email n√£o foi enviado")
            else:
                logger.warning("‚ö†Ô∏è Vari√°veis de email n√£o configuradas")
                logger.info("üìß Conte√∫do que seria enviado:")
                logger.info("-" * 40)
                logger.info(email_content[:500] + "...")
                logger.info("-" * 40)
        else:
            logger.warning("‚ö†Ô∏è Nenhuma oportunidade encontrada")
            
    except Exception as e:
        logger.error(f"‚ùå Erro cr√≠tico: {e}")
        
    finally:
        # Limpeza
        scraper.cleanup()

if __name__ == "__main__":
    main()

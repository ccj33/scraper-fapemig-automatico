#!/usr/bin/env python3
"""
Extrator de Dados de PDFs Melhorado
===================================

Extrai e analisa dados de PDFs de editais e chamadas
com melhor qualidade de texto e padr√µes mais robustos
"""

import requests
import PyPDF2
import io
import re
import logging
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlparse
import os
from datetime import datetime
import fitz  # PyMuPDF - mais robusto para PDFs complexos

# Configura√ß√£o de logging
logger = logging.getLogger(__name__)

class ExtratorPDF:
    """Classe para extrair dados de PDFs com melhor qualidade"""
    
    def __init__(self, diretorio_downloads: str = "downloads_pdf"):
        self.diretorio_downloads = diretorio_downloads
        self._criar_diretorio()
        
    def _criar_diretorio(self):
        """Cria diret√≥rio para downloads se n√£o existir"""
        if not os.path.exists(self.diretorio_downloads):
            os.makedirs(self.diretorio_downloads)
            logger.info(f"üìÅ Diret√≥rio criado: {self.diretorio_downloads}")
    
    def extrair_de_url(self, url: str, nome_arquivo: str = None) -> Dict:
        """
        Extrai dados de um PDF a partir de uma URL
        
        Args:
            url: URL do PDF
            nome_arquivo: Nome opcional para salvar o arquivo
            
        Returns:
            Dicion√°rio com dados extra√≠dos
        """
        try:
            logger.info(f"üîç Iniciando extra√ß√£o de PDF: {url}")
            
            # Verificar se √© um PDF
            if not self._eh_pdf(url):
                logger.warning(f"‚ö†Ô∏è URL n√£o parece ser um PDF: {url}")
                return {"erro": "URL n√£o √© um PDF v√°lido"}
            
            # Baixar PDF
            pdf_bytes = self._baixar_pdf(url)
            if not pdf_bytes:
                return {"erro": "Falha ao baixar PDF"}
            
            # Salvar arquivo localmente
            nome_arquivo = nome_arquivo or self._gerar_nome_arquivo(url)
            caminho_arquivo = os.path.join(self.diretorio_downloads, nome_arquivo)
            
            with open(caminho_arquivo, 'wb') as f:
                f.write(pdf_bytes)
            
            logger.info(f"üíæ PDF salvo: {caminho_arquivo}")
            
            # Extrair dados
            dados_extraidos = self._extrair_dados_pdf(pdf_bytes)
            dados_extraidos.update({
                'url_origem': url,
                'arquivo_local': caminho_arquivo,
                'tamanho_bytes': len(pdf_bytes),
                'data_extracao': datetime.now().isoformat()
            })
            
            logger.info(f"‚úÖ Extra√ß√£o conclu√≠da: {len(dados_extraidos)} campos extra√≠dos")
            return dados_extraidos
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair PDF: {e}")
            return {"erro": str(e)}
    
    def _eh_pdf(self, url: str) -> bool:
        """Verifica se a URL aponta para um PDF"""
        try:
            # Verificar extens√£o
            parsed = urlparse(url)
            path = parsed.path.lower()
            
            if path.endswith('.pdf'):
                return True
            
            # Verificar se cont√©m 'pdf' na URL
            if 'pdf' in url.lower():
                return True
                
            # Verificar headers HTTP
            try:
                response = requests.head(url, timeout=10)
                content_type = response.headers.get('content-type', '').lower()
                return 'pdf' in content_type or 'application/pdf' in content_type
            except:
                pass
                
            return False
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao verificar se √© PDF: {e}")
            return False
    
    def _baixar_pdf(self, url: str) -> Optional[bytes]:
        """Baixa PDF da URL"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            if response.content:
                logger.info(f"üì• PDF baixado: {len(response.content)} bytes")
                return response.content
            else:
                logger.error("‚ùå PDF vazio recebido")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao baixar PDF: {e}")
            return None
    
    def _gerar_nome_arquivo(self, url: str) -> str:
        """Gera nome de arquivo baseado na URL"""
        try:
            parsed = urlparse(url)
            nome = os.path.basename(parsed.path)
            
            if not nome or not nome.endswith('.pdf'):
                nome = f"edital_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
            
            # Limpar nome do arquivo
            nome = re.sub(r'[<>:"/\\|?*]', '_', nome)
            return nome
            
        except:
            return f"edital_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
    
    def _extrair_dados_pdf(self, pdf_bytes: bytes) -> Dict:
        """Extrai dados do conte√∫do do PDF"""
        dados = {}
        
        try:
            # Tentar com PyMuPDF primeiro (mais robusto)
            try:
                dados = self._extrair_com_pymupdf(pdf_bytes)
            except:
                # Fallback para PyPDF2
                dados = self._extrair_com_pypdf2(pdf_bytes)
            
            # An√°lise adicional do conte√∫do
            if dados.get('texto_completo'):
                dados.update(self._analisar_conteudo_melhorado(dados['texto_completo']))
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair dados do PDF: {e}")
            dados = {"erro_extracao": str(e)}
        
        return dados
    
    def _extrair_com_pymupdf(self, pdf_bytes: bytes) -> Dict:
        """Extrai dados usando PyMuPDF (mais robusto)"""
        try:
            doc = fitz.open(stream=pdf_bytes, filetype="pdf")
            dados = {
                'num_paginas': len(doc),
                'texto_completo': '',
                'metadados': doc.metadata
            }
            
            # Extrair texto de todas as p√°ginas com melhor formata√ß√£o
            texto_completo = []
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                texto_pagina = page.get_text("text")  # Usar modo "text" para melhor formata√ß√£o
                texto_completo.append(texto_pagina)
            
            dados['texto_completo'] = '\n'.join(texto_completo)
            doc.close()
            
            return dados
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è PyMuPDF falhou, tentando PyPDF2: {e}")
            raise e
    
    def _extrair_com_pypdf2(self, pdf_bytes: bytes) -> Dict:
        """Extrai dados usando PyPDF2 (fallback)"""
        try:
            pdf_file = io.BytesIO(pdf_bytes)
            reader = PyPDF2.PdfReader(pdf_file)
            
            dados = {
                'num_paginas': len(reader.pages),
                'texto_completo': '',
                'metadados': reader.metadata
            }
            
            # Extrair texto de todas as p√°ginas
            texto_completo = []
            for page in reader.pages:
                texto_pagina = page.extract_text()
                if texto_pagina:
                    texto_completo.append(texto_pagina)
            
            dados['texto_completo'] = '\n'.join(texto_completo)
            
            return dados
            
        except Exception as e:
            logger.error(f"‚ùå PyPDF2 tamb√©m falhou: {e}")
            raise e
    
    def _analisar_conteudo_melhorado(self, texto: str) -> Dict:
        """Analisa o conte√∫do extra√≠do com padr√µes mais robustos"""
        analise = {}
        
        try:
            # Limpar e normalizar texto
            texto_limpo = self._limpar_texto(texto)
            
            # Padr√µes melhorados para valores
            analise.update(self._extrair_valores_melhorado(texto_limpo))
            
            # Padr√µes melhorados para datas
            analise.update(self._extrair_datas_melhorado(texto_limpo))
            
            # Padr√µes melhorados para prazos
            analise.update(self._extrair_prazos_melhorado(texto_limpo))
            
            # Padr√µes melhorados para objetivos
            analise.update(self._extrair_objetivos_melhorado(texto_limpo))
            
            # Padr√µes melhorados para √°reas tem√°ticas
            analise.update(self._extrair_areas_melhorado(texto_limpo))
            
            # Estat√≠sticas do texto
            analise['estatisticas'] = {
                'total_caracteres': len(texto),
                'total_palavras': len(texto.split()),
                'total_linhas': len(texto.split('\n')),
                'caracteres_limpos': len(texto_limpo)
            }
            
            # Detectar idioma
            analise['idioma_detectado'] = self._detectar_idioma(texto_limpo)
            
            # Resumo do conte√∫do (primeiras linhas)
            linhas = texto_limpo.split('\n')
            analise['resumo_conteudo'] = '\n'.join(linhas[:10]) if len(linhas) > 10 else texto_limpo[:500]
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro na an√°lise de conte√∫do: {e}")
        
        return analise
    
    def _limpar_texto(self, texto: str) -> str:
        """Limpa e normaliza o texto extra√≠do"""
        if not texto:
            return ""
        
        # Remover caracteres especiais problem√°ticos
        texto = re.sub(r'[^\w\s\.,;:!?()\[\]{}"\'-]', ' ', texto)
        
        # Normalizar espa√ßos
        texto = re.sub(r'\s+', ' ', texto)
        
        # Normalizar quebras de linha
        texto = re.sub(r'\n\s*\n', '\n', texto)
        
        # Remover linhas muito curtas (provavelmente ru√≠do)
        linhas = texto.split('\n')
        linhas_limpas = [linha.strip() for linha in linhas if len(linha.strip()) > 3]
        
        return '\n'.join(linhas_limpas)
    
    def _extrair_valores_melhorado(self, texto: str) -> Dict:
        """Extrai valores com padr√µes mais robustos"""
        valores_encontrados = []
        
        # Padr√µes para valores em reais
        padroes_valor = [
            r'R\$\s*([\d.,]+)',  # R$ 50.000,00
            r'R\$\s*([\d]+(?:\.\d{3})*(?:,\d{2})?)',  # R$ 50.000,00 ou R$ 50000,00
            r'valor[:\s]*R\$\s*([\d.,]+)',  # valor: R$ 100.000,00
            r'at√©\s*R\$\s*([\d.,]+)',  # at√© R$ 100.000,00
            r'm√°ximo\s*R\$\s*([\d.,]+)',  # m√°ximo R$ 100.000,00
            r'limite\s*R\$\s*([\d.,]+)',  # limite R$ 100.000,00
            r'([\d.,]+)\s*reais?',  # 50.000,00 reais
            r'([\d.,]+)\s*mil\s*reais',  # 50 mil reais
            r'([\d.,]+)\s*milh√µes?\s*de\s*reais',  # 2 milh√µes de reais
        ]
        
        for padrao in padroes_valor:
            matches = re.findall(padrao, texto, re.IGNORECASE)
            for match in matches:
                if match and len(match) > 0:
                    valores_encontrados.append(match)
        
        # Padr√µes para valores em outras moedas ou formatos
        padroes_outros = [
            r'USD\s*([\d.,]+)',  # USD 50,000.00
            r'EUR\s*([\d.,]+)',  # EUR 50,000.00
            r'([\d.,]+)\s*d√≥lares?',  # 50,000 d√≥lares
            r'([\d.,]+)\s*euros?',  # 50,000 euros
        ]
        
        for padrao in padroes_outros:
            matches = re.findall(padrao, texto, re.IGNORECASE)
            for match in matches:
                if match and len(match) > 0:
                    valores_encontrados.append(f"{match} (outra moeda)")
        
        # Remover duplicatas e valores muito pequenos
        valores_unicos = []
        for valor in valores_encontrados:
            if valor not in valores_unicos and len(valor) > 1:
                valores_unicos.append(valor)
        
        return {'valores_encontrados': valores_unicos[:5]} if valores_unicos else {}
    
    def _extrair_datas_melhorado(self, texto: str) -> Dict:
        """Extrai datas com padr√µes mais robustos"""
        datas_encontradas = []
        
        # Padr√µes para datas brasileiras
        padroes_data_br = [
            r'(\d{1,2}/\d{1,2}/\d{4})',  # DD/MM/AAAA
            r'(\d{1,2}-\d{1,2}-\d{4})',  # DD-MM-AAAA
            r'(\d{1,2}\.\d{1,2}\.\d{4})',  # DD.MM.AAAA
            r'(\d{1,2}\s+de\s+\w+\s+de\s+\d{4})',  # 15 de agosto de 2025
            r'(\d{1,2}\s+\w+\s+\d{4})',  # 15 agosto 2025
        ]
        
        # Padr√µes para datas internacionais
        padroes_data_int = [
            r'(\d{4}-\d{1,2}-\d{1,2})',  # AAAA-MM-DD
            r'(\d{1,2}/\d{1,2}/\d{2})',  # DD/MM/AA
            r'(\d{1,2}-\d{1,2}-\d{2})',  # DD-MM-AA
        ]
        
        for padrao in padroes_data_br + padroes_data_int:
            matches = re.findall(padrao, texto)
            for match in matches:
                if match and len(match) > 5:
                    datas_encontradas.append(match)
        
        # Padr√µes para datas por extenso
        padroes_extenso = [
            r'(\d{1,2}\s+de\s+\w+\s+de\s+\d{4})',  # 15 de agosto de 2025
            r'(\w+\s+\d{1,2},\s+\d{4})',  # agosto 15, 2025
        ]
        
        for padrao in padroes_extenso:
            matches = re.findall(padrao, texto, re.IGNORECASE)
            for match in matches:
                if match and len(match) > 10:
                    datas_encontradas.append(match)
        
        # Remover duplicatas
        datas_unicas = list(set(datas_encontradas))
        return {'datas_encontradas': datas_unicas[:10]} if datas_unicas else {}
    
    def _extrair_prazos_melhorado(self, texto: str) -> Dict:
        """Extrai prazos com padr√µes mais robustos"""
        prazos_encontrados = []
        
        # Padr√µes para prazos
        padroes_prazo = [
            r'prazo.*?(\d{1,2}/\d{1,2}/\d{4})',  # prazo at√© 30/09/2025
            r'at√©.*?(\d{1,2}/\d{1,2}/\d{4})',  # at√© 30/09/2025
            r'vencimento.*?(\d{1,2}/\d{1,2}/\d{4})',  # vencimento 30/09/2025
            r'inscri√ß√µes.*?(\d{1,2}/\d{1,2}/\d{4})',  # inscri√ß√µes at√© 30/09/2025
            r'data\s+limite.*?(\d{1,2}/\d{1,2}/\d{4})',  # data limite 30/09/2025
            r'encerramento.*?(\d{1,2}/\d{1,2}/\d{4})',  # encerramento 30/09/2025
            r'fim.*?(\d{1,2}/\d{1,2}/\d{4})',  # fim 30/09/2025
            r'termino.*?(\d{1,2}/\d{1,2}/\d{4})',  # termino 30/09/2025
        ]
        
        for padrao in padroes_prazo:
            matches = re.findall(padrao, texto, re.IGNORECASE)
            for match in matches:
                if match and len(match) > 5:
                    prazos_encontrados.append(match)
        
        # Padr√µes para prazos por extenso
        padroes_extenso = [
            r'prazo.*?(\d{1,2}\s+de\s+\w+\s+de\s+\d{4})',  # prazo at√© 30 de setembro de 2025
            r'at√©.*?(\d{1,2}\s+de\s+\w+\s+de\s+\d{4})',  # at√© 30 de setembro de 2025
        ]
        
        for padrao in padroes_extenso:
            matches = re.findall(padrao, texto, re.IGNORECASE)
            for match in matches:
                if match and len(match) > 10:
                    prazos_encontrados.append(match)
        
        # Remover duplicatas
        prazos_unicos = list(set(prazos_encontrados))
        return {'prazos_encontrados': prazos_unicos[:5]} if prazos_unicos else {}
    
    def _extrair_objetivos_melhorado(self, texto: str) -> Dict:
        """Extrai objetivos com padr√µes mais robustos"""
        objetivos_encontrados = []
        
        # Padr√µes para objetivos
        padroes_objetivo = [
            r'Objetivo[:\s]*([^.\n]{20,200})',  # Objetivo: descri√ß√£o...
            r'Objetivos[:\s]*([^.\n]{20,200})',  # Objetivos: descri√ß√£o...
            r'Descri√ß√£o[:\s]*([^.\n]{20,200})',  # Descri√ß√£o: descri√ß√£o...
            r'Resumo[:\s]*([^.\n]{20,200})',  # Resumo: descri√ß√£o...
            r'Finalidade[:\s]*([^.\n]{20,200})',  # Finalidade: descri√ß√£o...
            r'Prop√≥sito[:\s]*([^.\n]{20,200})',  # Prop√≥sito: descri√ß√£o...
            r'Justificativa[:\s]*([^.\n]{20,200})',  # Justificativa: descri√ß√£o...
        ]
        
        for padrao in padroes_objetivo:
            matches = re.findall(padrao, texto, re.IGNORECASE)
            for match in matches:
                if match and len(match.strip()) > 20:
                    objetivo_limpo = match.strip()
                    objetivos_encontrados.append(objetivo_limpo)
        
        # Padr√µes para objetivos sem dois pontos
        padroes_sem_colon = [
            r'Objetivo\s+([^.\n]{20,200})',  # Objetivo descri√ß√£o...
            r'Objetivos\s+([^.\n]{20,200})',  # Objetivos descri√ß√£o...
        ]
        
        for padrao in padroes_sem_colon:
            matches = re.findall(padrao, texto, re.IGNORECASE)
            for match in matches:
                if match and len(match.strip()) > 20:
                    objetivo_limpo = match.strip()
                    objetivos_encontrados.append(objetivo_limpo)
        
        # Remover duplicatas e limitar tamanho
        objetivos_unicos = []
        for objetivo in objetivos_encontrados:
            if objetivo not in objetivos_unicos:
                # Limitar a 300 caracteres
                objetivo_limitado = objetivo[:300] + "..." if len(objetivo) > 300 else objetivo
                objetivos_unicos.append(objetivo_limitado)
        
        return {'objetivos_encontrados': objetivos_unicos[:3]} if objetivos_unicos else {}
    
    def _extrair_areas_melhorado(self, texto: str) -> Dict:
        """Extrai √°reas tem√°ticas com padr√µes mais robustos"""
        areas_encontradas = []
        
        # Padr√µes para √°reas tem√°ticas
        padroes_area = [
            r'√Årea[:\s]*([^.\n]{10,150})',  # √Årea: descri√ß√£o...
            r'√Åreas[:\s]*([^.\n]{10,150})',  # √Åreas: descri√ß√£o...
            r'Tema[:\s]*([^.\n]{10,150})',  # Tema: descri√ß√£o...
            r'Temas[:\s]*([^.\n]{10,150})',  # Temas: descri√ß√£o...
            r'Linha[:\s]*([^.\n]{10,150})',  # Linha: descri√ß√£o...
            r'Linhas[:\s]*([^.\n]{10,150})',  # Linhas: descri√ß√£o...
            r'Campo[:\s]*([^.\n]{10,150})',  # Campo: descri√ß√£o...
            r'Campos[:\s]*([^.\n]{10,150})',  # Campos: descri√ß√£o...
            r'Disciplina[:\s]*([^.\n]{10,150})',  # Disciplina: descri√ß√£o...
            r'Especialidade[:\s]*([^.\n]{10,150})',  # Especialidade: descri√ß√£o...
        ]
        
        for padrao in padroes_area:
            matches = re.findall(padrao, texto, re.IGNORECASE)
            for match in matches:
                if match and len(match.strip()) > 10:
                    area_limpa = match.strip()
                    areas_encontradas.append(area_limpa)
        
        # Padr√µes para √°reas sem dois pontos
        padroes_sem_colon = [
            r'√Årea\s+([^.\n]{10,150})',  # √Årea descri√ß√£o...
            r'Tema\s+([^.\n]{10,150})',  # Tema descri√ß√£o...
            r'Linha\s+([^.\n]{10,150})',  # Linha descri√ß√£o...
        ]
        
        for padrao in padroes_sem_colon:
            matches = re.findall(padrao, texto, re.IGNORECASE)
            for match in matches:
                if match and len(match.strip()) > 10:
                    area_limpa = match.strip()
                    areas_encontradas.append(area_limpa)
        
        # Remover duplicatas e limitar tamanho
        areas_unicas = []
        for area in areas_encontradas:
            if area not in areas_unicas:
                # Limitar a 200 caracteres
                area_limitada = area[:200] + "..." if len(area) > 200 else area
                areas_unicas.append(area_limitada)
        
        return {'areas_encontradas': areas_unicas[:3]} if areas_unicas else {}
    
    def _detectar_idioma(self, texto: str) -> str:
        """Detecta o idioma do texto"""
        if not texto:
            return "desconhecido"
        
        # Contar palavras em portugu√™s vs ingl√™s
        palavras_pt = len(re.findall(r'\b(de|para|com|por|em|n√£o|s√£o|est√°|ser|ter|que|uma|um|este|esta|como|mais|muito|pode|devem|todos|todas|outros|outras|primeiro|segundo|terceiro|quarto|quinto|sexto|s√©timo|oitavo|nono|d√©cimo)\b', texto, re.IGNORECASE))
        palavras_en = len(re.findall(r'\b(the|and|for|with|in|is|are|was|were|have|has|will|can|should|would|could|this|that|these|those|first|second|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth)\b', texto, re.IGNORECASE))
        
        if palavras_pt > palavras_en:
            return "portugu√™s"
        elif palavras_en > palavras_pt:
            return "ingl√™s"
        else:
            return "misto"
    
    def extrair_multiplos_pdfs(self, urls: List[str]) -> List[Dict]:
        """Extrai dados de m√∫ltiplos PDFs"""
        resultados = []
        
        for i, url in enumerate(urls, 1):
            logger.info(f"üìÑ Processando PDF {i}/{len(urls)}: {url}")
            
            resultado = self.extrair_de_url(url)
            resultados.append(resultado)
            
            # Delay para n√£o sobrecarregar servidores
            import time
            time.sleep(2)
        
        return resultados
    
    def limpar_arquivos_locais(self):
        """Remove arquivos PDF baixados localmente"""
        try:
            import shutil
            if os.path.exists(self.diretorio_downloads):
                shutil.rmtree(self.diretorio_downloads)
                logger.info(f"üóëÔ∏è Diret√≥rio limpo: {self.diretorio_downloads}")
        except Exception as e:
            logger.error(f"‚ùå Erro ao limpar arquivos: {e}")

def main():
    """Teste do extrator de PDFs melhorado"""
    extrator = ExtratorPDF()
    
    # URLs de exemplo para teste
    urls_teste = [
        "https://www.ufmg.br/prograd/wp-content/uploads/2024/01/edital_exemplo.pdf",
        "https://www.fapemig.br/wp-content/uploads/2024/01/chamada_exemplo.pdf"
    ]
    
    print("üß™ Testando extrator de PDFs melhorado...")
    
    for url in urls_teste:
        print(f"\nüîç Testando: {url}")
        resultado = extrator.extrair_de_url(url)
        
        if 'erro' not in resultado:
            print(f"‚úÖ P√°ginas: {resultado.get('num_paginas', 'N/A')}")
            print(f"üìä Caracteres: {resultado.get('estatisticas', {}).get('total_caracteres', 'N/A')}")
            print(f"üí∞ Valores encontrados: {resultado.get('valores_encontrados', 'N/A')}")
            print(f"üìÖ Datas encontradas: {resultado.get('datas_encontradas', 'N/A')}")
            print(f"‚è∞ Prazos encontrados: {resultado.get('prazos_encontrados', 'N/A')}")
            print(f"üéØ Objetivos encontrados: {resultado.get('objetivos_encontrados', 'N/A')}")
            print(f"üî¨ √Åreas encontradas: {resultado.get('areas_encontradas', 'N/A')}")
        else:
            print(f"‚ùå Erro: {resultado['erro']}")
    
    # Limpar arquivos de teste
    extrator.limpar_arquivos_locais()

if __name__ == "__main__":
    main()

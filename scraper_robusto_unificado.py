#!/usr/bin/env python3
"""
Scraper Robusto e Unificado para Editais e Chamadas
==================================================

Sistema completo que resolve todos os problemas cr√≠ticos identificados:
1. Captura de links diretos via Selenium
2. Download robusto com httpx e redirecionamentos
3. Extra√ß√£o robusta de PDFs com m√∫ltiplos fallbacks
4. Integra√ß√£o inteligente de dados
5. Gera√ß√£o de resumos completos sem truncamento
6. C√°lculo de hash SHA256 para deduplica√ß√£o
7. Campos link_pdf e pdf_hash inclu√≠dos
"""

import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import os
import time
import json
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
import chromedriver_autoinstaller
import logging
from typing import List, Dict, Optional, Tuple
import random

# Importar m√≥dulos robustos
from extrator_pdf_robusto import ExtratorPDFRobusto
from integrador_pdf_robusto import IntegradorPDFRobusto
from gerador_resumo_completo import GeradorResumoCompleto

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper_robusto.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BaseScraperRobusto:
    """Classe base robusta para todos os scrapers"""
    
    def __init__(self, driver: webdriver.Chrome):
        self.driver = driver
        self.wait = WebDriverWait(driver, 10)
        self.extrator_pdf = ExtratorPDFRobusto()
        
    def safe_find_element(self, by: By, value: str) -> Optional[webdriver.remote.webelement.WebElement]:
        """Encontra elemento de forma segura"""
        try:
            return self.driver.find_element(by, value)
        except NoSuchElementException:
            return None
            
    def safe_find_elements(self, by: By, value: str) -> List[webdriver.remote.webelement.WebElement]:
        """Encontra elementos de forma segura"""
        try:
            return self.driver.find_elements(by, value)
        except NoSuchElementException:
            return []
            
    def safe_get_text(self, element) -> str:
        """Extrai texto de forma segura"""
        try:
            return element.text.strip()
        except:
            return ""
            
    def safe_get_attribute(self, element, attribute: str) -> str:
        """Extrai atributo de forma segura"""
        try:
            return element.get_attribute(attribute) or ""
        except:
            return ""
            
    def random_delay(self, min_seconds: float = 1.0, max_seconds: float = 3.0):
        """Delay aleat√≥rio para n√£o sobrecarregar os sites"""
        delay = random.uniform(min_seconds, max_seconds)
        time.sleep(delay)
    
    def extrair_pdf_robusto(self, url: str, titulo: str) -> Dict:
        """Extrai PDF de forma robusta usando o extrator melhorado"""
        try:
            # Adicionar driver ao item para o extrator usar
            item_temp = {'driver': self.driver, 'url': url, 'titulo': titulo}
            
            # Usar o extrator robusto
            resultado = self.extrator_pdf.extrair_de_url_com_selenium(
                self.driver, url, titulo
            )
            
            return resultado
            
        except Exception as e:
            logger.error(f"‚ùå Erro na extra√ß√£o robusta de PDF: {e}")
            return {
                'status_baixa': 'erro_critico',
                'erro': str(e),
                'url_origem': url,
                'data_extracao': datetime.now().isoformat()
            }

class UFMGScraperRobusto(BaseScraperRobusto):
    """Scraper robusto para UFMG - Editais e Chamadas"""
    
    def __init__(self, driver: webdriver.Chrome):
        super().__init__(driver)
        self.base_url = "https://www.ufmg.br/prograd/editais-chamadas/"
        
    def extract_editais(self) -> List[Dict]:
        """Extrai editais da UFMG com funcionalidades robustas"""
        logger.info("üöÄ Iniciando extra√ß√£o robusta UFMG...")
        
        try:
            self.driver.get(self.base_url)
            self.random_delay(2, 4)
            
            editais = []
            page = 1
            
            while True:
                logger.info(f"üìÑ Processando p√°gina {page}...")
                
                # Extrair editais da p√°gina atual
                page_editais = self._extract_page_editais()
                editais.extend(page_editais)
                
                # Tentar ir para pr√≥xima p√°gina
                if not self._go_to_next_page():
                    break
                    
                page += 1
                self.random_delay(2, 4)
                
                # Limite de p√°ginas para n√£o sobrecarregar
                if page > 10:
                    logger.warning("‚ö†Ô∏è Limite de 10 p√°ginas atingido")
                    break
                    
            logger.info(f"‚úÖ UFMG: {len(editais)} editais extra√≠dos")
            return editais
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair UFMG: {e}")
            return []
            
    def _extract_page_editais(self) -> List[Dict]:
        """Extrai editais de uma p√°gina espec√≠fica com funcionalidades robustas"""
        editais = []
        
        try:
            # Procurar por links de editais (mais abrangente)
            edital_links = self.safe_find_elements(By.CSS_SELECTOR, 'a[href*="Edital"], a[href*="edital"], a[href*=".pdf"], a[href*=".doc"]')
            
            for link in edital_links:
                try:
                    titulo = self.safe_get_text(link)
                    href = self.safe_get_attribute(link, "href")
                    
                    if titulo and href and "edital" in titulo.lower():
                        # Procurar data pr√≥xima ao link
                        data = self._extract_date_near_link(link)
                        
                        # Extrair PDF de forma robusta se for um link de PDF
                        pdf_info = {}
                        if self._eh_url_pdf(href):
                            logger.info(f"üìÑ Extraindo PDF robusto: {titulo}")
                            pdf_info = self.extrair_pdf_robusto(href, titulo)
                        
                        edital = {
                            'titulo': titulo,
                            'url': href,
                            'data': data,
                            'fonte': 'UFMG',
                            'data_extracao': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            'driver': self.driver  # Para o extrator usar
                        }
                        
                        # Adicionar informa√ß√µes do PDF se dispon√≠vel
                        if pdf_info and 'erro' not in pdf_info:
                            edital.update({
                                'pdf_extraido': True,
                                'pdf_info': pdf_info
                            })
                        else:
                            edital.update({
                                'pdf_extraido': False,
                                'pdf_erro': pdf_info.get('erro', 'Falha na extra√ß√£o')
                            })
                        
                        editais.append(edital)
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao processar link UFMG: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair p√°gina UFMG: {e}")
            
        return editais
    
    def _eh_url_pdf(self, url: str) -> bool:
        """Verifica se a URL √© um PDF"""
        if not url:
            return False
        
        url_lower = url.lower()
        return '.pdf' in url_lower or 'pdf' in url_lower
        
    def _extract_date_near_link(self, link_element) -> str:
        """Extrai data pr√≥xima ao link do edital"""
        try:
            # Procurar por texto com data pr√≥ximo ao link
            parent = link_element.find_element(By.XPATH, "./..")
            parent_text = self.safe_get_text(parent)
            
            # Procurar padr√µes de data
            import re
            date_patterns = [
                r'\d{2}/\d{2}/\d{4}',
                r'\d{2}-\d{2}-\d{4}',
                r'\d{2}\.\d{2}\.\d{4}'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, parent_text)
                if match:
                    return match.group()
                    
        except:
            pass
            
        return "Data n√£o encontrada"
        
    def _go_to_next_page(self) -> bool:
        """Tenta ir para a pr√≥xima p√°gina"""
        try:
            # Procurar por links de pr√≥xima p√°gina
            next_links = self.safe_find_elements(By.XPATH, '//a[contains(text(), "Pr√≥xima") or contains(text(), "¬ª")]')
            
            for link in next_links:
                if link.is_enabled() and link.is_displayed():
                    link.click()
                    return True
                    
        except:
            pass
            
        return False

class FAPEMIGScraperRobusto(BaseScraperRobusto):
    """Scraper robusto para FAPEMIG - Oportunidades"""
    
    def __init__(self, driver: webdriver.Chrome):
        super().__init__(driver)
        self.base_url = "http://www.fapemig.br/pt/chamadas_abertas_oportunidades_fapemig/"
        
    def extract_chamadas(self) -> List[Dict]:
        """Extrai chamadas da FAPEMIG com funcionalidades robustas"""
        logger.info("üöÄ Iniciando extra√ß√£o robusta FAPEMIG...")
        
        try:
            self.driver.get(self.base_url)
            self.random_delay(3, 5)
            
            chamadas = []
            
            # Extrair chamadas da p√°gina principal
            page_chamadas = self._extract_page_chamadas()
            chamadas.extend(page_chamadas)
            
            # Tentar expandir detalhes se poss√≠vel
            chamadas = self._expand_chamadas_details(chamadas)
            
            logger.info(f"‚úÖ FAPEMIG: {len(chamadas)} chamadas extra√≠das")
            return chamadas
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair FAPEMIG: {e}")
            return []
            
    def _extract_page_chamadas(self) -> List[Dict]:
        """Extrai chamadas da p√°gina principal com funcionalidades robustas"""
        chamadas = []
        titulos_processados = set()  # Para evitar duplicatas
        
        try:
            # Procurar por t√≠tulos de chamadas de forma mais abrangente
            titulos = self.safe_find_elements(By.CSS_SELECTOR, 'h1, h2, h3, h4, h5, h6, .chamada, .edital, .oportunidade')
            
            for titulo in titulos:
                try:
                    texto = self.safe_get_text(titulo)
                    
                    if texto and any(palavra in texto.lower() for palavra in ['chamada', 'edital', 'oportunidade', 'programa']):
                        # Verificar se j√° processamos este t√≠tulo
                        if texto in titulos_processados:
                            continue
                        titulos_processados.add(texto)
                        
                        # Procurar link associado de forma mais robusta
                        link = self._find_link_near_title(titulo)
                        
                        # Se n√£o encontrou link pr√≥ximo, procurar na p√°gina inteira
                        if not link:
                            link = self._find_link_by_title_text(texto)
                        
                        # Extrair contexto da p√°gina para obter mais informa√ß√µes
                        contexto = self._extract_context_around_title(titulo)
                        
                        chamada = {
                            'titulo': texto,
                            'url': link,
                            'contexto': contexto,
                            'fonte': 'FAPEMIG',
                            'data_extracao': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            'driver': self.driver  # Para o extrator usar
                        }
                        
                        # Extrair PDF de forma robusta se for um link de PDF
                        if link and self._eh_url_pdf(link):
                            logger.info(f"üìÑ Extraindo PDF robusto FAPEMIG: {texto}")
                            pdf_info = self.extrair_pdf_robusto(link, texto)
                            
                            if 'erro' not in pdf_info:
                                chamada.update({
                                    'pdf_extraido': True,
                                    'pdf_info': pdf_info
                                })
                            else:
                                chamada.update({
                                    'pdf_extraido': False,
                                    'pdf_erro': pdf_info.get('erro', 'Falha na extra√ß√£o')
                                })
                        else:
                            chamada.update({
                                'pdf_extraido': False,
                                'pdf_motivo': 'Link n√£o √© PDF ou n√£o dispon√≠vel'
                            })
                        
                        chamadas.append(chamada)
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao processar t√≠tulo FAPEMIG: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair p√°gina FAPEMIG: {e}")
            
        return chamadas
    
    def _find_link_by_title_text(self, titulo_texto: str) -> str:
        """Encontra link baseado no texto do t√≠tulo"""
        try:
            # Procurar por links que contenham palavras do t√≠tulo
            palavras_chave = [palavra.lower() for palavra in titulo_texto.split() if len(palavra) > 3]
            
            links = self.safe_find_elements(By.TAG_NAME, "a")
            for link in links:
                href = self.safe_get_attribute(link, "href")
                texto_link = self.safe_get_text(link)
                
                if href and href.startswith("http"):
                    # Verificar se o link cont√©m palavras-chave do t√≠tulo
                    for palavra in palavras_chave:
                        if palavra in texto_link.lower() or palavra in href.lower():
                            return href
                            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao procurar link por t√≠tulo: {e}")
            
        return ""
    
    def _extract_context_around_title(self, titulo_element) -> str:
        """Extrai contexto ao redor do t√≠tulo para obter mais informa√ß√µes"""
        try:
            # Procurar por par√°grafo pr√≥ximo ao t√≠tulo
            parent = titulo_element.find_element(By.XPATH, "./..")
            
            # Procurar por par√°grafos pr√≥ximos
            paragrafos = parent.find_elements(By.TAG_NAME, "p")
            if paragrafos:
                contexto = " ".join([p.text.strip() for p in paragrafos[:3] if p.text.strip()])
                if contexto:
                    return contexto
            
            # Se n√£o encontrou par√°grafos, procurar por divs com texto
            divs = parent.find_elements(By.TAG_NAME, "div")
            for div in divs[:3]:
                texto = div.text.strip()
                if texto and len(texto) > 20:  # Texto significativo
                    return texto
                    
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao extrair contexto: {e}")
            
        return ""
        
    def _eh_url_pdf(self, url: str) -> bool:
        """Verifica se a URL √© um PDF"""
        if not url:
            return False
        
        url_lower = url.lower()
        return '.pdf' in url_lower or 'pdf' in url_lower
        
    def _find_link_near_title(self, titulo_element) -> str:
        """Encontra link pr√≥ximo ao t√≠tulo"""
        try:
            # Procurar por link pr√≥ximo ao t√≠tulo
            parent = titulo_element.find_element(By.XPATH, "./..")
            links = parent.find_elements(By.TAG_NAME, "a")
            
            for link in links:
                href = self.safe_get_attribute(link, "href")
                texto_link = self.safe_get_text(link)
                
                # Priorizar links que parecem ser PDFs ou editais
                if href and href.startswith("http"):
                    if any(ext in href.lower() for ext in ['.pdf', '.doc', '.docx']):
                        return href
                    if any(palavra in texto_link.lower() for palavra in ['edital', 'chamada', 'pdf', 'download']):
                        return href
                    return href  # Retorna o primeiro link v√°lido
                    
        except:
            pass
            
        # Se n√£o encontrou no parent, procurar em toda a p√°gina
        try:
            links_globais = self.driver.find_elements(By.TAG_NAME, "a")
            for link in links_globais:
                href = self.safe_get_attribute(link, "href")
                texto_link = self.safe_get_text(link)
                
                if href and href.startswith("http") and any(palavra in texto_link.lower() for palavra in ['edital', 'chamada', 'pdf']):
                    return href
                    
        except:
            pass
            
        return ""
        
    def _expand_chamadas_details(self, chamadas: List[Dict]) -> List[Dict]:
        """Tenta expandir detalhes das chamadas"""
        expanded_chamadas = []
        
        for chamada in chamadas:
            try:
                if chamada.get('url') and not self._eh_url_pdf(chamada['url']):
                    # Tentar acessar p√°gina de detalhes (apenas se n√£o for PDF direto)
                    logger.info(f"üîç Expandindo detalhes FAPEMIG: {chamada.get('titulo', 'Sem t√≠tulo')}")
                    self.driver.get(chamada['url'])
                    self.random_delay(2, 3)
                    
                    # Extrair informa√ß√µes adicionais
                    detalhes = self._extract_chamada_details()
                    chamada.update(detalhes)
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao expandir detalhes FAPEMIG: {e}")
                
            expanded_chamadas.append(chamada)
            
        return expanded_chamadas
        
    def _extract_chamada_details(self) -> Dict:
        """Extrai detalhes de uma chamada espec√≠fica"""
        detalhes = {}
        
        try:
            # Procurar por valores/recursos de forma mais abrangente
            valor_elements = self.safe_find_elements(By.XPATH, 
                '//*[contains(text(), "R$") or contains(text(), "valor") or contains(text(), "recurso") or contains(text(), "investimento")]')
            if valor_elements:
                detalhes['valor'] = self.safe_get_text(valor_elements[0])
                
            # Procurar por datas de forma mais abrangente
            data_elements = self.safe_find_elements(By.XPATH, 
                '//*[contains(text(), "data") or contains(text(), "prazo") or contains(text(), "per√≠odo") or contains(text(), "inscri√ß√£o") or contains(text(), "submiss√£o")]')
            if data_elements:
                detalhes['prazo'] = self.safe_get_text(data_elements[0])
            
            # Procurar por objetivos
            objetivo_elements = self.safe_find_elements(By.XPATH, 
                '//*[contains(text(), "objetivo") or contains(text(), "finalidade") or contains(text(), "p√∫blico")]')
            if objetivo_elements:
                detalhes['objetivo'] = self.safe_get_text(objetivo_elements[0])
            
            # Procurar por √°reas
            area_elements = self.safe_find_elements(By.XPATH, 
                '//*[contains(text(), "√°rea") or contains(text(), "campo") or contains(text(), "setor")]')
            if area_elements:
                detalhes['area'] = self.safe_get_text(area_elements[0])
            
            # Extrair texto da p√°gina para an√°lise posterior
            body_text = self.safe_find_element(By.TAG_NAME, "body")
            if body_text:
                texto_pagina = self.safe_get_text(body_text)
                if texto_pagina:
                    detalhes['texto_pagina'] = texto_pagina[:2000]  # Primeiros 2000 caracteres
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao extrair detalhes FAPEMIG: {e}")
            
        return detalhes

class CNPqScraperRobusto(BaseScraperRobusto):
    """Scraper robusto para CNPq - Chamadas P√∫blicas"""
    
    def __init__(self, driver: webdriver.Chrome):
        super().__init__(driver)
        self.base_url = "http://memoria2.cnpq.br/web/guest/chamadas-publicas"
        
    def extract_chamadas(self) -> List[Dict]:
        """Extrai chamadas do CNPq com funcionalidades robustas"""
        logger.info("üöÄ Iniciando extra√ß√£o robusta CNPq...")
        
        try:
            self.driver.get(self.base_url)
            self.random_delay(3, 5)
            
            chamadas = []
            
            # Extrair chamadas da p√°gina principal
            page_chamadas = self._extract_page_chamadas()
            chamadas.extend(page_chamadas)
            
            # Tentar expandir detalhes
            chamadas = self._expand_chamadas_details(chamadas)
            
            logger.info(f"‚úÖ CNPq: {len(chamadas)} chamadas extra√≠das")
            return chamadas
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair CNPq: {e}")
            return []
            
    def _extract_page_chamadas(self) -> List[Dict]:
        """Extrai chamadas da p√°gina principal com funcionalidades robustas"""
        chamadas = []
        
        try:
            # Procurar por t√≠tulos de chamadas de forma mais abrangente
            titulos = self.safe_find_elements(By.XPATH, 
                '//h4[contains(text(), "CHAMADA") or contains(text(), "Chamada") or contains(text(), "PROGRAMA") or contains(text(), "Programa")]')
            
            # Se n√£o encontrou t√≠tulos espec√≠ficos, procurar por outros elementos
            if not titulos:
                titulos = self.safe_find_elements(By.CSS_SELECTOR, 'h1, h2, h3, h4, h5, h6, .chamada, .programa')
            
            for titulo in titulos:
                try:
                    texto = self.safe_get_text(titulo)
                    
                    if texto and any(palavra in texto.lower() for palavra in ['chamada', 'programa', 'edital', 'bolsa', 'aux√≠lio']):
                        # Procurar per√≠odo de inscri√ß√£o
                        periodo = self._find_periodo_near_title(titulo)
                        
                        # Procurar link para detalhes
                        link_detalhes = self._find_link_detalhes_near_title(titulo)
                        
                        # Extrair contexto ao redor do t√≠tulo
                        contexto = self._extract_context_around_title(titulo)
                        
                        # Extrair PDF de forma robusta se for um link de PDF
                        pdf_info = {}
                        if link_detalhes and self._eh_url_pdf(link_detalhes):
                            logger.info(f"üìÑ Extraindo PDF robusto CNPq: {texto}")
                            pdf_info = self.extrair_pdf_robusto(link_detalhes, texto)
                        
                        chamada = {
                            'titulo': texto,
                            'periodo_inscricao': periodo,
                            'url_detalhes': link_detalhes,
                            'contexto': contexto,
                            'fonte': 'CNPq',
                            'data_extracao': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            'driver': self.driver  # Para o extrator usar
                        }
                        
                        # Adicionar informa√ß√µes do PDF se dispon√≠vel
                        if pdf_info and 'erro' not in pdf_info:
                            chamada.update({
                                'pdf_extraido': True,
                                'pdf_info': pdf_info
                            })
                        else:
                            chamada.update({
                                'pdf_extraido': False,
                                'pdf_motivo': 'Link n√£o √© PDF ou n√£o dispon√≠vel'
                            })
                        
                        chamadas.append(chamada)
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao processar t√≠tulo CNPq: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair p√°gina CNPq: {e}")
            
        return chamadas
    
    def _eh_url_pdf(self, url: str) -> bool:
        """Verifica se a URL √© um PDF"""
        if not url:
            return False
        
        url_lower = url.lower()
        return '.pdf' in url_lower or 'pdf' in url_lower
        
    def _find_periodo_near_title(self, titulo_element) -> str:
        """Encontra per√≠odo de inscri√ß√£o pr√≥ximo ao t√≠tulo"""
        try:
            # Procurar por texto com per√≠odo pr√≥ximo ao t√≠tulo
            parent = titulo_element.find_element(By.XPATH, "./..")
            parent_text = self.safe_get_text(parent)
            
            # Procurar padr√µes de per√≠odo
            import re
            periodo_patterns = [
                r'Inscri√ß√µes:\s*\d{2}/\d{2}/\d{4}\s*a\s*\d{2}/\d{2}/\d{4}',
                r'Per√≠odo:\s*\d{2}/\d{2}/\d{4}\s*a\s*\d{2}/\d{2}/\d{4}',
                r'\d{2}/\d{2}/\d{4}\s*a\s*\d{2}/\d{2}/\d{4}'
            ]
            
            for pattern in periodo_patterns:
                match = re.search(pattern, parent_text)
                if match:
                    return match.group()
                    
        except:
            pass
            
        return "Per√≠odo n√£o encontrado"
        
    def _find_link_detalhes_near_title(self, titulo_element) -> str:
        """Encontra link para detalhes pr√≥ximo ao t√≠tulo"""
        try:
            # Procurar por link pr√≥ximo ao t√≠tulo
            parent = titulo_element.find_element(By.XPATH, "./..")
            links = parent.find_elements(By.TAG_NAME, "a")
            
            for link in links:
                href = self.safe_get_attribute(link, "href")
                texto = self.safe_get_text(link)
                
                if href and ("chamada" in texto.lower() or "detalhes" in texto.lower() or "pdf" in texto.lower()):
                    return href
                    
        except:
            pass
            
        # Se n√£o encontrou no parent, procurar em toda a p√°gina
        try:
            links_globais = self.driver.find_elements(By.TAG_NAME, "a")
            for link in links_globais:
                href = self.safe_get_attribute(link, "href")
                texto = self.safe_get_text(link)
                
                if href and href.startswith("http") and any(palavra in texto.lower() for palavra in ['chamada', 'detalhes', 'pdf', 'edital']):
                    return href
                    
        except:
            pass
            
        return ""
        
    def _expand_chamadas_details(self, chamadas: List[Dict]) -> List[Dict]:
        """Tenta expandir detalhes das chamadas do CNPq"""
        expanded_chamadas = []
        
        for chamada in chamadas:
            try:
                if chamada.get('url_detalhes') and not self._eh_url_pdf(chamada['url_detalhes']):
                    # Tentar acessar p√°gina de detalhes (apenas se n√£o for PDF direto)
                    logger.info(f"üîç Expandindo detalhes CNPq: {chamada.get('titulo', 'Sem t√≠tulo')}")
                    self.driver.get(chamada['url_detalhes'])
                    self.random_delay(2, 3)
                    
                    # Extrair informa√ß√µes adicionais
                    detalhes = self._extract_cnpq_details()
                    chamada.update(detalhes)
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao expandir detalhes CNPq: {e}")
                
            expanded_chamadas.append(chamada)
            
        return expanded_chamadas
        
    def _extract_cnpq_details(self) -> Dict:
        """Extrai detalhes de uma chamada espec√≠fica do CNPq"""
        detalhes = {}
        
        try:
            # Procurar por valores/recursos
            valor_elements = self.safe_find_elements(By.XPATH, 
                '//*[contains(text(), "R$") or contains(text(), "valor") or contains(text(), "recurso") or contains(text(), "investimento")]')
            if valor_elements:
                detalhes['valor'] = self.safe_get_text(valor_elements[0])
                
            # Procurar por datas de forma mais abrangente
            data_elements = self.safe_find_elements(By.XPATH, 
                '//*[contains(text(), "data") or contains(text(), "prazo") or contains(text(), "per√≠odo") or contains(text(), "inscri√ß√£o") or contains(text(), "submiss√£o")]')
            if data_elements:
                detalhes['prazo'] = self.safe_get_text(data_elements[0])
            
            # Procurar por objetivos
            objetivo_elements = self.safe_find_elements(By.XPATH, 
                '//*[contains(text(), "objetivo") or contains(text(), "finalidade") or contains(text(), "p√∫blico") or contains(text(), "destinat√°rio")]')
            if objetivo_elements:
                detalhes['objetivo'] = self.safe_get_text(objetivo_elements[0])
            
            # Procurar por √°reas
            area_elements = self.safe_find_elements(By.XPATH, 
                '//*[contains(text(), "√°rea") or contains(text(), "campo") or contains(text(), "setor") or contains(text(), "grande √°rea")]')
            if area_elements:
                detalhes['area'] = self.safe_get_text(area_elements[0])
            
            # Extrair texto da p√°gina para an√°lise posterior
            body_text = self.safe_find_element(By.TAG_NAME, "body")
            if body_text:
                texto_pagina = self.safe_get_text(body_text)
                if texto_pagina:
                    detalhes['texto_pagina'] = texto_pagina[:2000]  # Primeiros 2000 caracteres
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao extrair detalhes CNPq: {e}")
            
        return detalhes

class ScraperRobustoUnificado:
    """Classe principal robusta que coordena todos os scrapers"""
    
    def __init__(self):
        self.driver = None
        self.results = {
            'ufmg': [],
            'fapemig': [],
            'cnpq': [],
            'timestamp': datetime.now().isoformat(),
            'total_editais': 0
        }
        self.integrador = IntegradorPDFRobusto()
        self.gerador_resumo = None
        
    def setup_driver(self):
        """Configura o driver do Chrome"""
        try:
            # Instalar ChromeDriver
            chromedriver_autoinstaller.install()
            
            # Configura√ß√µes para modo headless
            options = Options()
            options.add_argument("--headless")
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("--disable-gpu")
            options.add_argument("--window-size=1920,1080")
            options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
            
            self.driver = webdriver.Chrome(options=options)
            logger.info("‚úÖ Driver Chrome configurado com sucesso")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao configurar driver: {e}")
            raise
            
    def run_scraping(self):
        """Executa o scraping robusto de todos os sites"""
        try:
            logger.info("üöÄ Iniciando scraping robusto unificado...")
            
            # UFMG
            try:
                ufmg_scraper = UFMGScraperRobusto(self.driver)
                self.results['ufmg'] = ufmg_scraper.extract_editais()
                logger.info(f"‚úÖ UFMG: {len(self.results['ufmg'])} editais extra√≠dos")
            except Exception as e:
                logger.error(f"‚ùå Erro UFMG: {e}")
                self.results['ufmg'] = []
                
            # FAPEMIG
            try:
                fapemig_scraper = FAPEMIGScraperRobusto(self.driver)
                self.results['fapemig'] = fapemig_scraper.extract_chamadas()
                logger.info(f"‚úÖ FAPEMIG: {len(self.results['fapemig'])} chamadas extra√≠das")
            except Exception as e:
                logger.error(f"‚ùå Erro FAPEMIG: {e}")
                self.results['fapemig'] = []
                
            # CNPq
            try:
                cnpq_scraper = CNPqScraperRobusto(self.driver)
                self.results['cnpq'] = cnpq_scraper.extract_chamadas()
                logger.info(f"‚úÖ CNPq: {len(self.results['cnpq'])} chamadas extra√≠das")
            except Exception as e:
                logger.error(f"‚ùå Erro CNPq: {e}")
                self.results['cnpq'] = []
                
            # Calcular total
            self.results['total_editais'] = (
                len(self.results['ufmg']) + 
                len(self.results['fapemig']) + 
                len(self.results['cnpq'])
            )
            
            logger.info(f"üéâ Scraping robusto conclu√≠do! Total: {self.results['total_editais']} oportunidades")
            
        except Exception as e:
            logger.error(f"‚ùå Erro geral no scraping: {e}")
    
    def integrar_pdfs(self):
        """Integra PDFs extra√≠dos usando o integrador robusto"""
        try:
            logger.info("üîó Iniciando integra√ß√£o robusta de PDFs...")
            
            # Usar o integrador robusto
            self.results = self.integrador.processar_editais_com_pdfs(self.results)
            
            logger.info("‚úÖ Integra√ß√£o de PDFs conclu√≠da com sucesso")
            
        except Exception as e:
            logger.error(f"‚ùå Erro na integra√ß√£o de PDFs: {e}")
    
    def gerar_resumo_completo(self):
        """Gera resumo completo usando o gerador robusto"""
        try:
            logger.info("üìù Iniciando gera√ß√£o de resumo completo...")
            
            # Usar o gerador robusto
            self.gerador_resumo = GeradorResumoCompleto(self.results)
            resumo = self.gerador_resumo.gerar_resumo_completo()
            
            logger.info("‚úÖ Resumo completo gerado com sucesso")
            return resumo
            
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o de resumo: {e}")
            return "Erro ao gerar resumo"
            
    def save_results(self):
        """Salva os resultados em arquivos"""
        try:
            # Remover campos n√£o serializ√°veis antes de salvar
            results_clean = self._clean_results_for_serialization()
            
            # Salvar JSON completo
            with open("resultados_robustos_completos.json", "w", encoding="utf-8") as f:
                json.dump(results_clean, f, ensure_ascii=False, indent=2)
                
            # Salvar resumo em texto
            resumo = self.gerar_resumo_completo()
            with open("resumo_scraping_robusto.txt", "w", encoding="utf-8") as f:
                f.write(resumo)
                
            logger.info("‚úÖ Resultados robustos salvos com sucesso")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar resultados: {e}")
    
    def _clean_results_for_serialization(self) -> Dict:
        """Remove campos n√£o serializ√°veis dos resultados"""
        results_clean = self.results.copy()
        
        # Remover campos driver de todos os editais/chamadas
        for fonte in ['ufmg', 'fapemig', 'cnpq']:
            if fonte in results_clean:
                for item in results_clean[fonte]:
                    if 'driver' in item:
                        del item['driver']
        
        return results_clean
        
    def cleanup(self):
        """Limpa recursos"""
        if self.driver:
            try:
                self.driver.quit()
                logger.info("‚úÖ Driver fechado com sucesso")
            except:
                pass
        
        # Limpar arquivos PDF baixados
        try:
            self.integrador.limpar_arquivos_pdf()
            logger.info("‚úÖ Arquivos PDF limpos com sucesso")
        except:
            pass

def send_email_robusto(msg_text: str, subject: str = "üöÄ Scraper Robusto - Novas Oportunidades!"):
    """Envia email com os resultados robustos"""
    try:
        msg = MIMEMultipart()
        msg['Subject'] = subject
        msg['From'] = os.environ['EMAIL_USER']
        msg['To'] = os.environ.get('EMAIL_DESTINO', 'clevioferreira@gmail.com')
        
        # Corpo do email
        text_part = MIMEText(msg_text, 'plain', 'utf-8')
        msg.attach(text_part)
        
        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:
            server.login(os.environ['EMAIL_USER'], os.environ['EMAIL_PASS'])
            server.send_message(msg)
            
        logger.info(f"‚úÖ Email robusto enviado com sucesso para {msg['To']}!")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao enviar email: {e}")
        return False

def main():
    """Fun√ß√£o principal"""
    logger.info("=" * 60)
    logger.info("üöÄ SCRAPER ROBUSTO UNIFICADO - EDITAIS E CHAMADAS")
    logger.info("=" * 60)
    
    scraper = ScraperRobustoUnificado()
    
    try:
        # Configurar driver
        scraper.setup_driver()
        
        # Executar scraping robusto
        scraper.run_scraping()
        
        # Integrar PDFs extra√≠dos
        scraper.integrar_pdfs()
        
        # Salvar resultados
        scraper.save_results()
        
        # Preparar email
        if scraper.results['total_editais'] > 0:
            email_content = scraper.gerar_resumo_completo()
            
            # Enviar email se as vari√°veis estiverem configuradas
            if 'EMAIL_USER' in os.environ and 'EMAIL_PASS' in os.environ:
                logger.info("üìß Enviando email robusto...")
                if send_email_robusto(email_content):
                    logger.info("‚úÖ Processo robusto conclu√≠do com sucesso!")
                else:
                    logger.warning("‚ö†Ô∏è Scraping robusto conclu√≠do, mas email n√£o foi enviado")
            else:
                logger.warning("‚ö†Ô∏è Vari√°veis de email n√£o configuradas")
                logger.info("üìß Conte√∫do que seria enviado:")
                logger.info("-" * 40)
                logger.info(email_content)
                logger.info("-" * 40)
        else:
            logger.warning("‚ö†Ô∏è Nenhuma oportunidade encontrada")
            
    except Exception as e:
        logger.error(f"‚ùå Erro cr√≠tico: {e}")
        
    finally:
        # Limpeza
        scraper.cleanup()

if __name__ == "__main__":
    main()

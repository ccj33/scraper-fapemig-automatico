#!/usr/bin/env python3
"""
Scraper Melhorado - ExtraÃ§Ã£o Completa de Editais e Chamadas
==========================================================

Coleta dados COMPLETOS de:
- UFMG: Editais e Chamadas com PDFs
- FAPEMIG: Oportunidades com detalhes
- CNPq: Chamadas PÃºblicas com prazos

VersÃ£o 2.0 - ExtraÃ§Ã£o inteligente e legÃ­vel
"""

import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import os
import time
import json
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
import chromedriver_autoinstaller
import logging
import re
from typing import List, Dict, Optional
import random

# ConfiguraÃ§Ã£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper_melhorado.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BaseScraperMelhorado:
    """Classe base melhorada para todos os scrapers"""
    
    def __init__(self, driver: webdriver.Chrome):
        self.driver = driver
        self.wait = WebDriverWait(driver, 15)
        
    def safe_find_element(self, by: By, value: str):
        """Encontra elemento de forma segura"""
        try:
            return self.driver.find_element(by, value)
        except NoSuchElementException:
            return None
            
    def safe_find_elements(self, by: By, value: str):
        """Encontra elementos de forma segura"""
        try:
            return self.driver.find_elements(by, value)
        except NoSuchElementException:
            return []
            
    def safe_get_text(self, element) -> str:
        """Extrai texto de forma segura"""
        try:
            return element.text.strip()
        except:
            return ""
            
    def safe_get_attribute(self, element, attribute: str) -> str:
        """Extrai atributo de forma segura"""
        try:
            return element.get_attribute(attribute) or ""
        except:
            return ""
            
    def random_delay(self, min_seconds: float = 2.0, max_seconds: float = 4.0):
        """Delay aleatÃ³rio para nÃ£o sobrecarregar os sites"""
        delay = random.uniform(min_seconds, max_seconds)
        time.sleep(delay)
        
    def extract_pdf_content(self, pdf_url: str) -> Dict:
        """Tenta extrair conteÃºdo de PDFs"""
        try:
            logger.info(f"ðŸ“„ Tentando extrair PDF: {pdf_url}")
            
            # Acessar URL do PDF
            self.driver.get(pdf_url)
            self.random_delay(3, 5)
            
            # Tentar extrair texto da pÃ¡gina
            page_text = self.driver.page_source
            
            # Procurar por informaÃ§Ãµes importantes
            info = {
                'valor': self._extract_valor_from_text(page_text),
                'prazo': self._extract_prazo_from_text(page_text),
                'objetivo': self._extract_objetivo_from_text(page_text),
                'publico_alvo': self._extract_publico_from_text(page_text)
            }
            
            return info
            
        except Exception as e:
            logger.warning(f"âš ï¸ Erro ao extrair PDF: {e}")
            return {}
            
    def _extract_valor_from_text(self, text: str) -> str:
        """Extrai valores do texto"""
        patterns = [
            r'R\$\s*[\d.,]+',
            r'valor.*?R\$\s*[\d.,]+',
            r'recurso.*?R\$\s*[\d.,]+',
            r'bolsa.*?R\$\s*[\d.,]+'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group().strip()
        return "Valor nÃ£o informado"
        
    def _extract_prazo_from_text(self, text: str) -> str:
        """Extrai prazos do texto"""
        patterns = [
            r'prazo.*?\d{2}/\d{2}/\d{4}',
            r'atÃ©.*?\d{2}/\d{2}/\d{4}',
            r'encerramento.*?\d{2}/\d{2}/\d{4}',
            r'\d{2}/\d{2}/\d{4}.*?prazo'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group().strip()
        return "Prazo nÃ£o informado"
        
    def _extract_objetivo_from_text(self, text: str) -> str:
        """Extrai objetivo do texto"""
        patterns = [
            r'objetivo.*?[.!]',
            r'finalidade.*?[.!]',
            r'proposta.*?[.!]'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group().strip()
        return "Objetivo nÃ£o informado"
        
    def _extract_publico_from_text(self, text: str) -> str:
        """Extrai pÃºblico-alvo do texto"""
        patterns = [
            r'pÃºblico.*?[.!]',
            r'destinatÃ¡rios.*?[.!]',
            r'beneficiÃ¡rios.*?[.!]'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group().strip()
        return "PÃºblico nÃ£o informado"

class UFMGScraperMelhorado(BaseScraperMelhorado):
    """Scraper melhorado para UFMG"""
    
    def __init__(self, driver: webdriver.Chrome):
        super().__init__(driver)
        self.base_url = "https://www.ufmg.br/prograd/editais-chamadas/"
        
    def extract_editais(self) -> List[Dict]:
        """Extrai editais da UFMG com informaÃ§Ãµes completas"""
        logger.info("ðŸš€ Iniciando extraÃ§Ã£o UFMG melhorada...")
        
        try:
            self.driver.get(self.base_url)
            self.random_delay(3, 5)
            
            editais = []
            page = 1
            
            while page <= 5:  # Limite de 5 pÃ¡ginas
                logger.info(f"ðŸ“„ Processando pÃ¡gina {page}...")
                
                page_editais = self._extract_page_editais()
                editais.extend(page_editais)
                
                if not self._go_to_next_page():
                    break
                    
                page += 1
                self.random_delay(3, 5)
                
            logger.info(f"âœ… UFMG: {len(editais)} editais extraÃ­dos")
            return editais
            
        except Exception as e:
            logger.error(f"âŒ Erro ao extrair UFMG: {e}")
            return []
            
    def _extract_page_editais(self) -> List[Dict]:
        """Extrai editais de uma pÃ¡gina com informaÃ§Ãµes completas"""
        editais = []
        
        try:
            # Procurar por links de editais
            edital_links = self.safe_find_elements(By.CSS_SELECTOR, 'a[href*=".pdf"], a[href*="edital"], a[href*="Edital"]')
            
            for link in edital_links:
                try:
                    titulo = self.safe_get_text(link)
                    href = self.safe_get_attribute(link, "href")
                    
                    if titulo and href and ("edital" in titulo.lower() or "pdf" in href.lower()):
                        # Extrair informaÃ§Ãµes completas
                        info_completa = self._extract_edital_info(link, titulo, href)
                        editais.append(info_completa)
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ Erro ao processar link UFMG: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"âŒ Erro ao extrair pÃ¡gina UFMG: {e}")
            
        return editais
        
    def _extract_edital_info(self, link_element, titulo: str, href: str) -> Dict:
        """Extrai informaÃ§Ãµes completas do edital"""
        info = {
            'titulo_completo': titulo,
            'url': href,
            'fonte': 'UFMG',
            'data_extracao': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        try:
            # Procurar data prÃ³xima ao link
            data = self._extract_date_near_link(link_element)
            info['data'] = data
            
            # Se for PDF, tentar extrair conteÃºdo
            if href.lower().endswith('.pdf'):
                pdf_info = self.extract_pdf_content(href)
                info.update(pdf_info)
                
        except Exception as e:
            logger.warning(f"âš ï¸ Erro ao extrair detalhes UFMG: {e}")
            
        return info
        
    def _extract_date_near_link(self, link_element) -> str:
        """Extrai data prÃ³xima ao link do edital"""
        try:
            # Procurar por texto com data prÃ³ximo ao link
            parent = link_element.find_element(By.XPATH, "./..")
            parent_text = self.safe_get_text(parent)
            
            # PadrÃµes de data mais especÃ­ficos
            date_patterns = [
                r'\d{2}/\d{2}/\d{4}',
                r'\d{2}-\d{2}-\d{4}',
                r'\d{2}\.\d{2}\.\d{4}'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, parent_text)
                if match:
                    return match.group()
                    
        except:
            pass
            
        return "Data nÃ£o encontrada"
        
    def _go_to_next_page(self) -> bool:
        """Tenta ir para a prÃ³xima pÃ¡gina"""
        try:
            next_links = self.safe_find_elements(By.XPATH, '//a[contains(text(), "PrÃ³xima") or contains(text(), "Â»")]')
            
            for link in next_links:
                if link.is_enabled() and link.is_displayed():
                    link.click()
                    return True
                    
        except:
            pass
            
        return False

def main():
    """FunÃ§Ã£o principal para teste"""
    logger.info("=" * 60)
    logger.info("ðŸš€ SCRAPER MELHORADO - TESTE")
    logger.info("=" * 60)
    
    # Por enquanto, apenas teste bÃ¡sico
    logger.info("âœ… Scraper melhorado criado com sucesso!")
    logger.info("ðŸ“‹ PrÃ³ximos passos:")
    logger.info("   1. Implementar FAPEMIG e CNPq melhorados")
    logger.info("   2. Adicionar extraÃ§Ã£o de PDFs")
    logger.info("   3. Melhorar formataÃ§Ã£o do resumo")
    logger.info("   4. Testar no GitHub Actions")

if __name__ == "__main__":
    main()

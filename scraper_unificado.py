#!/usr/bin/env python3
"""
Scraper Unificado para Editais e Chamadas
=========================================

Coleta dados de:
- UFMG: Editais e Chamadas
- FAPEMIG: Oportunidades
- CNPq: Chamadas P√∫blicas

Desenvolvido seguindo as melhores pr√°ticas de web scraping
"""

import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import os
import time
import json
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
import chromedriver_autoinstaller
import logging
from typing import List, Dict, Optional, Tuple
import random
import requests
import os
import PyPDF2
import fitz  # PyMuPDF

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BaseScraper:
    """Classe base para todos os scrapers"""
    
    def __init__(self, driver: webdriver.Chrome):
        self.driver = driver
        self.wait = WebDriverWait(driver, 10)
        
    def safe_find_element(self, by: By, value: str) -> Optional[webdriver.remote.webelement.WebElement]:
        """Encontra elemento de forma segura"""
        try:
            return self.driver.find_element(by, value)
        except NoSuchElementException:
            return None
            
    def safe_find_elements(self, by: By, value: str) -> List[webdriver.remote.webelement.WebElement]:
        """Encontra elementos de forma segura"""
        try:
            return self.driver.find_elements(by, value)
        except NoSuchElementException:
            return []
            
    def safe_get_text(self, element) -> str:
        """Extrai texto de forma segura"""
        try:
            return element.text.strip()
        except:
            return ""
            
    def safe_get_attribute(self, element, attribute: str) -> str:
        """Extrai atributo de forma segura"""
        try:
            return element.get_attribute(attribute) or ""
        except:
            return ""
            
    def random_delay(self, min_seconds: float = 1.0, max_seconds: float = 3.0):
        """Delay aleat√≥rio para n√£o sobrecarregar os sites"""
        delay = random.uniform(min_seconds, max_seconds)
        time.sleep(delay)
        
    def download_pdf_if_available(self, url: str, titulo: str) -> str:
        """Baixa PDF se dispon√≠vel e retorna o caminho do arquivo"""
        try:
            if not url:
                return ""
                
            # Verificar se √© um arquivo que pode ser baixado
            if not any(ext in url.lower() for ext in ['.pdf', '.doc', '.docx']):
                # Se n√£o √© arquivo direto, tentar encontrar PDF na p√°gina
                return self._find_and_download_pdf_from_page(url, titulo)
                
            # Criar diret√≥rio para PDFs se n√£o existir
            pdf_dir = "pdfs_baixados"
            if not os.path.exists(pdf_dir):
                os.makedirs(pdf_dir)
                
            # Nome do arquivo baseado no t√≠tulo
            safe_title = "".join(c for c in titulo if c.isalnum() or c in (' ', '-', '_')).rstrip()
            safe_title = safe_title[:50]  # Limitar tamanho
            filename = f"{pdf_dir}/{safe_title}.pdf"
            
            # Baixar PDF
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            with open(filename, 'wb') as f:
                f.write(response.content)
                
            logger.info(f"‚úÖ PDF baixado: {filename}")
            return filename
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao baixar PDF {url}: {e}")
            return ""
            
    def _find_and_download_pdf_from_page(self, url: str, titulo: str) -> str:
        """Tenta encontrar e baixar PDF de uma p√°gina espec√≠fica"""
        try:
            # Acessar a p√°gina para procurar PDFs
            self.driver.get(url)
            self.random_delay(2, 3)
            
            # Procurar por links de PDF
            pdf_links = self.safe_find_elements(By.CSS_SELECTOR, 'a[href*=".pdf"], a[href*=".doc"], a[href*=".docx"]')
            
            for link in pdf_links:
                href = self.safe_get_attribute(link, "href")
                if href and href.startswith("http"):
                    # Tentar baixar este PDF
                    return self._download_specific_pdf(href, titulo)
                    
            # Se n√£o encontrou, procurar por texto que sugira PDF
            text_elements = self.safe_find_elements(By.XPATH, '//*[contains(text(), "PDF") or contains(text(), "Download") or contains(text(), "Edital")]')
            for element in text_elements:
                parent = element.find_element(By.XPATH, "./..")
                links = parent.find_elements(By.TAG_NAME, "a")
                for link in links:
                    href = self.safe_get_attribute(link, "href")
                    if href and href.startswith("http"):
                        return self._download_specific_pdf(href, titulo)
                        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao procurar PDF na p√°gina {url}: {e}")
            
        return ""
        
    def _download_specific_pdf(self, url: str, titulo: str) -> str:
        """Baixa um PDF espec√≠fico"""
        try:
            # Criar diret√≥rio para PDFs se n√£o existir
            pdf_dir = "pdfs_baixados"
            if not os.path.exists(pdf_dir):
                os.makedirs(pdf_dir)
                
            # Nome do arquivo baseado no t√≠tulo
            safe_title = "".join(c for c in titulo if c.isalnum() or c in (' ', '-', '_')).rstrip()
            safe_title = safe_title[:50]  # Limitar tamanho
            filename = f"{pdf_dir}/{safe_title}.pdf"
            
            # Baixar PDF
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            with open(filename, 'wb') as f:
                f.write(response.content)
                
            logger.info(f"‚úÖ PDF encontrado e baixado: {filename}")
            return filename
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao baixar PDF espec√≠fico {url}: {e}")
            return ""
            
    def extract_pdf_content(self, pdf_path: str) -> Dict:
        """Extrai conte√∫do e informa√ß√µes detalhadas de um PDF"""
        if not pdf_path or not os.path.exists(pdf_path):
            return {}
            
        try:
            logger.info(f"üìñ Lendo conte√∫do do PDF: {pdf_path}")
            
            # Tentar com PyMuPDF primeiro (mais robusto)
            try:
                return self._extract_with_pymupdf(pdf_path)
            except:
                # Fallback para PyPDF2
                try:
                    return self._extract_with_pypdf2(pdf_path)
                except:
                    logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel ler o PDF: {pdf_path}")
                    return {}
                    
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair conte√∫do do PDF {pdf_path}: {e}")
            return {}
            
    def _extract_with_pymupdf(self, pdf_path: str) -> Dict:
        """Extrai conte√∫do usando PyMuPDF"""
        doc = fitz.open(pdf_path)
        texto_completo = ""
        
        for page_num in range(len(doc)):
            page = doc.load_page(page_num)
            texto_completo += page.get_text()
            
        doc.close()
        
        return self._analyze_pdf_text(texto_completo)
        
    def _extract_with_pypdf2(self, pdf_path: str) -> Dict:
        """Extrai conte√∫do usando PyPDF2"""
        with open(pdf_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            texto_completo = ""
            
            for page in reader.pages:
                texto_completo += page.extract_text()
                
        return self._analyze_pdf_text(texto_completo)
        
    def _analyze_pdf_text(self, texto: str) -> Dict:
        """Analisa o texto extra√≠do do PDF para encontrar informa√ß√µes importantes"""
        import re
        
        info = {}
        
        # Padr√µes para datas
        date_patterns = [
            r'(\d{2}/\d{2}/\d{4})',  # DD/MM/AAAA
            r'(\d{2}-\d{2}-\d{4})',  # DD-MM-AAAA
            r'(\d{2}\.\d{2}\.\d{4})', # DD.MM.AAAA
            r'(\d{1,2}\s+de\s+\w+\s+de\s+\d{4})', # 15 de agosto de 2025
        ]
        
        datas_encontradas = []
        for pattern in date_patterns:
            matches = re.findall(pattern, texto)
            datas_encontradas.extend(matches)
            
        if datas_encontradas:
            info['datas_encontradas'] = list(set(datas_encontradas))  # Remove duplicatas
            
        # Padr√µes para valores/recursos
        valor_patterns = [
            r'R\$\s*([\d.,]+)',  # R$ 50.000,00
            r'(\d+\.?\d*)\s*mil\s*reais',  # 50 mil reais
            r'(\d+\.?\d*)\s*milh√µes?\s*de\s*reais',  # 2 milh√µes de reais
            r'Valor:\s*R\$\s*([\d.,]+)',  # Valor: R$ 100.000,00
        ]
        
        valores_encontrados = []
        for pattern in valor_patterns:
            matches = re.findall(pattern, texto, re.IGNORECASE)
            valores_encontrados.extend(matches)
            
        if valores_encontrados:
            info['valores_encontrados'] = valores_encontrados
            
        # Padr√µes para prazos
        prazo_patterns = [
            r'prazo.*?(\d{2}/\d{2}/\d{4})',  # prazo at√© 30/09/2025
            r'at√©.*?(\d{2}/\d{2}/\d{4})',    # at√© 30/09/2025
            r'vencimento.*?(\d{2}/\d{2}/\d{4})', # vencimento 30/09/2025
            r'inscri√ß√µes.*?(\d{2}/\d{2}/\d{4})', # inscri√ß√µes at√© 30/09/2025
        ]
        
        prazos_encontrados = []
        for pattern in prazo_patterns:
            matches = re.findall(pattern, texto, re.IGNORECASE)
            prazos_encontrados.extend(matches)
            
        if prazos_encontrados:
            info['prazos_encontrados'] = list(set(prazos_encontrados))
            
        # Padr√µes para objetivos/descri√ß√£o
        objetivo_patterns = [
            r'Objetivo[:\s]*([^.\n]+)',
            r'Objetivos[:\s]*([^.\n]+)',
            r'Descri√ß√£o[:\s]*([^.\n]+)',
            r'Resumo[:\s]*([^.\n]+)',
        ]
        
        for pattern in objetivo_patterns:
            match = re.search(pattern, texto, re.IGNORECASE)
            if match:
                info['objetivo'] = match.group(1).strip()
                break
                
        # Padr√µes para √°rea/tema
        area_patterns = [
            r'√Årea[:\s]*([^.\n]+)',
            r'Tema[:\s]*([^.\n]+)',
            r'Linha[:\s]*([^.\n]+)',
            r'Campo[:\s]*([^.\n]+)',
        ]
        
        for pattern in area_patterns:
            match = re.search(pattern, texto, re.IGNORECASE)
            if match:
                info['area_tema'] = match.group(1).strip()
                break
                
        # Contar p√°ginas e tamanho
        info['tamanho_texto'] = len(texto)
        info['palavras'] = len(texto.split())
        
        logger.info(f"‚úÖ PDF analisado: {len(info)} informa√ß√µes extra√≠das")
        return info

class UFMGScraper(BaseScraper):
    """Scraper para UFMG - Editais e Chamadas"""
    
    def __init__(self, driver: webdriver.Chrome):
        super().__init__(driver)
        self.base_url = "https://www.ufmg.br/prograd/editais-chamadas/"
        
    def extract_editais(self) -> List[Dict]:
        """Extrai editais da UFMG"""
        logger.info("üöÄ Iniciando extra√ß√£o UFMG...")
        
        try:
            self.driver.get(self.base_url)
            self.random_delay(2, 4)
            
            editais = []
            page = 1
            
            while True:
                logger.info(f"üìÑ Processando p√°gina {page}...")
                
                # Extrair editais da p√°gina atual
                page_editais = self._extract_page_editais()
                editais.extend(page_editais)
                
                # Tentar ir para pr√≥xima p√°gina
                if not self._go_to_next_page():
                    break
                    
                page += 1
                self.random_delay(2, 4)
                
                # Limite de p√°ginas para n√£o sobrecarregar
                if page > 10:
                    logger.warning("‚ö†Ô∏è Limite de 10 p√°ginas atingido")
                    break
                    
            logger.info(f"‚úÖ UFMG: {len(editais)} editais extra√≠dos")
            return editais
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair UFMG: {e}")
            return []
            
    def _extract_page_editais(self) -> List[Dict]:
        """Extrai editais de uma p√°gina espec√≠fica"""
        editais = []
        
        try:
            # Procurar por links de editais (mais abrangente)
            edital_links = self.safe_find_elements(By.CSS_SELECTOR, 'a[href*="Edital"], a[href*="edital"], a[href*=".pdf"], a[href*=".doc"]')
            
            for link in edital_links:
                try:
                    titulo = self.safe_get_text(link)
                    href = self.safe_get_attribute(link, "href")
                    
                    if titulo and href and "edital" in titulo.lower():
                        # Procurar data pr√≥xima ao link
                        data = self._extract_date_near_link(link)
                        
                        # Tentar baixar PDF se for um link de PDF
                        pdf_path = self.download_pdf_if_available(href, titulo)
                        
                        # Se PDF foi baixado, extrair conte√∫do detalhado
                        pdf_info = {}
                        if pdf_path:
                            pdf_info = self.extract_pdf_content(pdf_path)
                        
                        edital = {
                            'titulo': titulo,
                            'url': href,
                            'data': data,
                            'fonte': 'UFMG',
                            'pdf_baixado': pdf_path,
                            'pdf_info': pdf_info,
                            'data_extracao': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        }
                        
                        editais.append(edital)
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao processar link UFMG: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair p√°gina UFMG: {e}")
            
        return editais
        
    def _extract_date_near_link(self, link_element) -> str:
        """Extrai data pr√≥xima ao link do edital"""
        try:
            # Procurar por texto com data pr√≥ximo ao link
            parent = link_element.find_element(By.XPATH, "./..")
            parent_text = self.safe_get_text(parent)
            
            # Procurar padr√µes de data
            import re
            date_patterns = [
                r'\d{2}/\d{2}/\d{4}',
                r'\d{2}-\d{2}-\d{4}',
                r'\d{2}\.\d{2}\.\d{4}'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, parent_text)
                if match:
                    return match.group()
                    
        except:
            pass
            
        return "Data n√£o encontrada"
        
    def _go_to_next_page(self) -> bool:
        """Tenta ir para a pr√≥xima p√°gina"""
        try:
            # Procurar por links de pr√≥xima p√°gina
            next_links = self.safe_find_elements(By.XPATH, '//a[contains(text(), "Pr√≥xima") or contains(text(), "¬ª")]')
            
            for link in next_links:
                if link.is_enabled() and link.is_displayed():
                    link.click()
                    return True
                    
        except:
            pass
            
        return False

class FAPEMIGScraper(BaseScraper):
    """Scraper para FAPEMIG - Oportunidades"""
    
    def __init__(self, driver: webdriver.Chrome):
        super().__init__(driver)
        self.base_url = "http://www.fapemig.br/pt/chamadas_abertas_oportunidades_fapemig/"
        
    def extract_chamadas(self) -> List[Dict]:
        """Extrai chamadas da FAPEMIG"""
        logger.info("üöÄ Iniciando extra√ß√£o FAPEMIG...")
        
        try:
            self.driver.get(self.base_url)
            self.random_delay(3, 5)
            
            chamadas = []
            
            # Extrair chamadas da p√°gina principal
            page_chamadas = self._extract_page_chamadas()
            chamadas.extend(page_chamadas)
            
            # Tentar expandir detalhes se poss√≠vel
            chamadas = self._expand_chamadas_details(chamadas)
            
            logger.info(f"‚úÖ FAPEMIG: {len(chamadas)} chamadas extra√≠das")
            return chamadas
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair FAPEMIG: {e}")
            return []
            
    def _extract_page_chamadas(self) -> List[Dict]:
        """Extrai chamadas da p√°gina principal"""
        chamadas = []
        titulos_processados = set()  # Para evitar duplicatas
        
        try:
            # Procurar por t√≠tulos de chamadas
            titulos = self.safe_find_elements(By.CSS_SELECTOR, 'h1, h2, h3, h4, h5, h6')
            
            for titulo in titulos:
                try:
                    texto = self.safe_get_text(titulo)
                    
                    if texto and any(palavra in texto.lower() for palavra in ['chamada', 'edital', 'oportunidade']):
                        # Verificar se j√° processamos este t√≠tulo
                        if texto in titulos_processados:
                            continue
                        titulos_processados.add(texto)
                        
                        # Procurar link associado
                        link = self._find_link_near_title(titulo)
                        
                        # Tentar baixar PDF se for um link de PDF
                        pdf_path = self.download_pdf_if_available(link, texto)
                        
                        # Se PDF foi baixado, extrair conte√∫do detalhado
                        pdf_info = {}
                        if pdf_path:
                            pdf_info = self.extract_pdf_content(pdf_path)
                        
                        chamada = {
                            'titulo': texto,
                            'url': link,
                            'fonte': 'FAPEMIG',
                            'pdf_baixado': pdf_path,
                            'pdf_info': pdf_info,
                            'data_extracao': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        }
                        
                        chamadas.append(chamada)
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao processar t√≠tulo FAPEMIG: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair p√°gina FAPEMIG: {e}")
            
        return chamadas
        
    def _find_link_near_title(self, titulo_element) -> str:
        """Encontra link pr√≥ximo ao t√≠tulo"""
        try:
            # Procurar por link pr√≥ximo ao t√≠tulo
            parent = titulo_element.find_element(By.XPATH, "./..")
            links = parent.find_elements(By.TAG_NAME, "a")
            
            for link in links:
                href = self.safe_get_attribute(link, "href")
                texto_link = self.safe_get_text(link)
                
                # Priorizar links que parecem ser PDFs ou editais
                if href and href.startswith("http"):
                    if any(ext in href.lower() for ext in ['.pdf', '.doc', '.docx']):
                        return href
                    if any(palavra in texto_link.lower() for palavra in ['edital', 'chamada', 'pdf', 'download']):
                        return href
                    return href  # Retorna o primeiro link v√°lido
                    
        except:
            pass
            
        # Se n√£o encontrou no parent, procurar em toda a p√°gina
        try:
            links_globais = self.driver.find_elements(By.TAG_NAME, "a")
            for link in links_globais:
                href = self.safe_get_attribute(link, "href")
                texto_link = self.safe_get_text(link)
                
                if href and href.startswith("http") and any(palavra in texto_link.lower() for palavra in ['edital', 'chamada', 'pdf']):
                    return href
                    
        except:
            pass
            
        return ""
        
    def _expand_chamadas_details(self, chamadas: List[Dict]) -> List[Dict]:
        """Tenta expandir detalhes das chamadas"""
        expanded_chamadas = []
        
        for chamada in chamadas:
            try:
                if chamada['url']:
                    # Tentar acessar p√°gina de detalhes
                    self.driver.get(chamada['url'])
                    self.random_delay(2, 3)
                    
                    # Extrair informa√ß√µes adicionais
                    detalhes = self._extract_chamada_details()
                    chamada.update(detalhes)
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao expandir detalhes FAPEMIG: {e}")
                
            expanded_chamadas.append(chamada)
            
        return expanded_chamadas
        
    def _extract_chamada_details(self) -> Dict:
        """Extrai detalhes de uma chamada espec√≠fica"""
        detalhes = {}
        
        try:
            # Procurar por valores/recursos
            valor_elements = self.safe_find_elements(By.XPATH, '//*[contains(text(), "R$") or contains(text(), "valor") or contains(text(), "recurso")]')
            if valor_elements:
                detalhes['valor'] = self.safe_get_text(valor_elements[0])
                
            # Procurar por datas
            data_elements = self.safe_find_elements(By.XPATH, '//*[contains(text(), "data") or contains(text(), "prazo") or contains(text(), "per√≠odo")]')
            if data_elements:
                detalhes['prazo'] = self.safe_get_text(data_elements[0])
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao extrair detalhes FAPEMIG: {e}")
            
        return detalhes

class CNPqScraper(BaseScraper):
    """Scraper para CNPq - Chamadas P√∫blicas"""
    
    def __init__(self, driver: webdriver.Chrome):
        super().__init__(driver)
        self.base_url = "http://memoria2.cnpq.br/web/guest/chamadas-publicas"
        
    def extract_chamadas(self) -> List[Dict]:
        """Extrai chamadas do CNPq"""
        logger.info("üöÄ Iniciando extra√ß√£o CNPq...")
        
        try:
            self.driver.get(self.base_url)
            self.random_delay(3, 5)
            
            chamadas = []
            
            # Extrair chamadas da p√°gina principal
            page_chamadas = self._extract_page_chamadas()
            chamadas.extend(page_chamadas)
            
            # Tentar expandir detalhes
            chamadas = self._expand_chamadas_details(chamadas)
            
            logger.info(f"‚úÖ CNPq: {len(chamadas)} chamadas extra√≠das")
            return chamadas
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair CNPq: {e}")
            return []
            
    def _extract_page_chamadas(self) -> List[Dict]:
        """Extrai chamadas da p√°gina principal"""
        chamadas = []
        
        try:
            # Procurar por t√≠tulos de chamadas
            titulos = self.safe_find_elements(By.XPATH, '//h4[contains(text(), "CHAMADA") or contains(text(), "Chamada")]')
            
            for titulo in titulos:
                try:
                    texto = self.safe_get_text(titulo)
                    
                    if texto:
                        # Procurar per√≠odo de inscri√ß√£o
                        periodo = self._find_periodo_near_title(titulo)
                        
                        # Procurar link para detalhes
                        link_detalhes = self._find_link_detalhes_near_title(titulo)
                        
                        # Tentar baixar PDF se for um link de PDF
                        pdf_path = self.download_pdf_if_available(link_detalhes, texto)
                        
                        # Se PDF foi baixado, extrair conte√∫do detalhado
                        pdf_info = {}
                        if pdf_path:
                            pdf_info = self.extract_pdf_content(pdf_path)
                        
                        chamada = {
                            'titulo': texto,
                            'periodo_inscricao': periodo,
                            'url_detalhes': link_detalhes,
                            'fonte': 'CNPq',
                            'pdf_baixado': pdf_path,
                            'pdf_info': pdf_info,
                            'data_extracao': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        }
                        
                        chamadas.append(chamada)
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao processar t√≠tulo CNPq: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair p√°gina CNPq: {e}")
            
        return chamadas
        
    def _find_periodo_near_title(self, titulo_element) -> str:
        """Encontra per√≠odo de inscri√ß√£o pr√≥ximo ao t√≠tulo"""
        try:
            # Procurar por texto com per√≠odo pr√≥ximo ao t√≠tulo
            parent = titulo_element.find_element(By.XPATH, "./..")
            parent_text = self.safe_get_text(parent)
            
            # Procurar padr√µes de per√≠odo
            import re
            periodo_patterns = [
                r'Inscri√ß√µes:\s*\d{2}/\d{2}/\d{4}\s*a\s*\d{2}/\d{2}/\d{4}',
                r'Per√≠odo:\s*\d{2}/\d{2}/\d{4}\s*a\s*\d{2}/\d{2}/\d{4}',
                r'\d{2}/\d{2}/\d{4}\s*a\s*\d{2}/\d{2}/\d{4}'
            ]
            
            for pattern in periodo_patterns:
                match = re.search(pattern, parent_text)
                if match:
                    return match.group()
                    
        except:
            pass
            
        return "Per√≠odo n√£o encontrado"
        
    def _find_link_detalhes_near_title(self, titulo_element) -> str:
        """Encontra link para detalhes pr√≥ximo ao t√≠tulo"""
        try:
            # Procurar por link pr√≥ximo ao t√≠tulo
            parent = titulo_element.find_element(By.XPATH, "./..")
            links = parent.find_elements(By.TAG_NAME, "a")
            
            for link in links:
                href = self.safe_get_attribute(link, "href")
                texto = self.safe_get_text(link)
                
                if href and ("chamada" in texto.lower() or "detalhes" in texto.lower() or "pdf" in texto.lower()):
                    return href
                    
        except:
            pass
            
        # Se n√£o encontrou no parent, procurar em toda a p√°gina
        try:
            links_globais = self.driver.find_elements(By.TAG_NAME, "a")
            for link in links_globais:
                href = self.safe_get_attribute(link, "href")
                texto = self.safe_get_text(link)
                
                if href and href.startswith("http") and any(palavra in texto.lower() for palavra in ['chamada', 'detalhes', 'pdf', 'edital']):
                    return href
                    
        except:
            pass
            
        return ""
        
    def _expand_chamadas_details(self, chamadas: List[Dict]) -> List[Dict]:
        """Tenta expandir detalhes das chamadas"""
        expanded_chamadas = []
        
        for chamada in chamadas:
            try:
                if chamada['url_detalhes']:
                    # Tentar acessar p√°gina de detalhes
                    self.driver.get(chamada['url_detalhes'])
                    self.random_delay(2, 3)
                    
                    # Extrair informa√ß√µes adicionais
                    detalhes = self._extract_chamada_details()
                    chamada.update(detalhes)
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao expandir detalhes CNPq: {e}")
                
            expanded_chamadas.append(chamada)
            
        return expanded_chamadas
        
    def _extract_chamada_details(self) -> Dict:
        """Extrai detalhes de uma chamada espec√≠fica"""
        detalhes = {}
        
        try:
            # Procurar por valores/recursos
            valor_elements = self.safe_find_elements(By.XPATH, '//*[contains(text(), "R$") or contains(text(), "valor") or contains(text(), "recurso")]')
            if valor_elements:
                detalhes['valor'] = self.safe_get_text(valor_elements[0])
                
            # Procurar por descri√ß√£o
            desc_elements = self.safe_find_elements(By.XPATH, '//p[contains(text(), "Objetivo") or contains(text(), "Descri√ß√£o")]')
            if desc_elements:
                detalhes['descricao'] = self.safe_get_text(desc_elements[0])
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao extrair detalhes CNPq: {e}")
            
        return detalhes

class ScraperUnificado:
    """Classe principal que coordena todos os scrapers"""
    
    def __init__(self):
        self.driver = None
        self.results = {
            'ufmg': [],
            'fapemig': [],
            'cnpq': [],
            'timestamp': datetime.now().isoformat(),
            'total_editais': 0
        }
        
    def setup_driver(self):
        """Configura o driver do Chrome"""
        try:
            # Instalar ChromeDriver
            chromedriver_autoinstaller.install()
            
            # Configura√ß√µes para modo headless
            options = Options()
            options.add_argument("--headless")
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("--disable-gpu")
            options.add_argument("--window-size=1920,1080")
            options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
            
            self.driver = webdriver.Chrome(options=options)
            logger.info("‚úÖ Driver Chrome configurado com sucesso")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao configurar driver: {e}")
            raise
            
    def run_scraping(self):
        """Executa o scraping de todos os sites"""
        try:
            logger.info("üöÄ Iniciando scraping unificado...")
            
            # UFMG
            try:
                ufmg_scraper = UFMGScraper(self.driver)
                self.results['ufmg'] = ufmg_scraper.extract_editais()
                logger.info(f"‚úÖ UFMG: {len(self.results['ufmg'])} editais extra√≠dos")
            except Exception as e:
                logger.error(f"‚ùå Erro UFMG: {e}")
                self.results['ufmg'] = []
                
            # FAPEMIG
            try:
                fapemig_scraper = FAPEMIGScraper(self.driver)
                self.results['fapemig'] = fapemig_scraper.extract_chamadas()
                logger.info(f"‚úÖ FAPEMIG: {len(self.results['fapemig'])} chamadas extra√≠das")
            except Exception as e:
                logger.error(f"‚ùå Erro FAPEMIG: {e}")
                self.results['fapemig'] = []
                
            # CNPq
            try:
                cnpq_scraper = CNPqScraper(self.driver)
                self.results['cnpq'] = cnpq_scraper.extract_chamadas()
                logger.info(f"‚úÖ CNPq: {len(self.results['cnpq'])} chamadas extra√≠das")
            except Exception as e:
                logger.error(f"‚ùå Erro CNPq: {e}")
                self.results['cnpq'] = []
                
            # Calcular total
            self.results['total_editais'] = (
                len(self.results['ufmg']) + 
                len(self.results['fapemig']) + 
                len(self.results['cnpq'])
            )
            
            logger.info(f"üéâ Scraping conclu√≠do! Total: {self.results['total_editais']} oportunidades")
            
        except Exception as e:
            logger.error(f"‚ùå Erro geral no scraping: {e}")
            
    def save_results(self):
        """Salva os resultados em arquivos"""
        try:
            # Salvar JSON completo
            with open("resultados_completos.json", "w", encoding="utf-8") as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
                
            # Salvar resumo em texto
            with open("resumo_scraping.txt", "w", encoding="utf-8") as f:
                f.write(self._generate_summary())
                
            logger.info("‚úÖ Resultados salvos com sucesso")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar resultados: {e}")
            
    def _generate_summary(self) -> str:
        """Gera resumo em texto dos resultados"""
        # Contar PDFs baixados e analisados
        pdfs_ufmg = sum(1 for e in self.results['ufmg'] if e.get('pdf_baixado'))
        pdfs_fapemig = sum(1 for e in self.results['fapemig'] if e.get('pdf_baixado'))
        pdfs_cnpq = sum(1 for e in self.results['cnpq'] if e.get('pdf_baixado'))
        total_pdfs = pdfs_ufmg + pdfs_fapemig + pdfs_cnpq
        
        # Contar PDFs com conte√∫do extra√≠do
        pdfs_analisados = sum(1 for e in self.results['ufmg'] + self.results['fapemig'] + self.results['cnpq'] if e.get('pdf_info'))
        
        summary = f"""
üöÄ RESUMO DO SCRAPING UNIFICADO
================================
Data/Hora: {datetime.now().strftime('%d/%m/%Y √†s %H:%M:%S')}
Total de Oportunidades: {self.results['total_editais']}
üìÑ PDFs Baixados: {total_pdfs}
üìñ PDFs Analisados: {pdfs_analisados}

üìä UFMG - Editais e Chamadas
-----------------------------
Total: {len(self.results['ufmg'])} editais
PDFs: {pdfs_ufmg} baixados
{self._format_editais_list(self.results['ufmg'])}

üìä FAPEMIG - Oportunidades
---------------------------
Total: {len(self.results['fapemig'])} chamadas
PDFs: {pdfs_fapemig} baixados
{self._format_editais_list(self.results['fapemig'])}

üìä CNPq - Chamadas P√∫blicas
----------------------------
Total: {len(self.results['cnpq'])} chamadas
PDFs: {pdfs_cnpq} baixados
{self._format_editais_list(self.results['cnpq'])}

---
ü§ñ Scraper automatizado executado via GitHub Actions
üìÖ Executado em: {datetime.now().strftime('%d/%m/%Y √†s %H:%M:%S')}
        """
        return summary
        
    def _format_editais_list(self, editais: List[Dict]) -> str:
        """Formata lista de editais para o resumo"""
        if not editais:
            return "Nenhuma oportunidade encontrada"
            
        formatted = ""
        for i, edital in enumerate(editais[:5], 1):  # Mostrar apenas os 5 primeiros
            titulo = edital.get('titulo', 'Sem t√≠tulo')
            # Mostrar t√≠tulo completo se for menor que 80 caracteres
            if len(titulo) <= 80:
                formatted += f"{i}. {titulo}\n"
            else:
                formatted += f"{i}. {titulo[:80]}...\n"
            
            # Adicionar informa√ß√µes extras se dispon√≠veis
            if edital.get('data'):
                formatted += f"   üìÖ Data: {edital['data']}\n"
            if edital.get('periodo_inscricao'):
                formatted += f"   üìÖ Per√≠odo: {edital['periodo_inscricao']}\n"
            if edital.get('valor'):
                formatted += f"   üí∞ Valor: {edital['valor']}\n"
                
            # Informa√ß√µes extra√≠das do PDF
            if edital.get('pdf_info'):
                pdf_info = edital['pdf_info']
                
                if pdf_info.get('datas_encontradas'):
                    formatted += f"   üìÖ Datas no PDF: {', '.join(pdf_info['datas_encontradas'][:3])}\n"
                if pdf_info.get('prazos_encontrados'):
                    formatted += f"   ‚è∞ Prazos: {', '.join(pdf_info['prazos_encontrados'][:2])}\n"
                if pdf_info.get('valores_encontrados'):
                    formatted += f"   üí∞ Valores: {', '.join(pdf_info['valores_encontrados'][:2])}\n"
                if pdf_info.get('objetivo'):
                    formatted += f"   üéØ Objetivo: {pdf_info['objetivo'][:80]}...\n"
                if pdf_info.get('area_tema'):
                    formatted += f"   üî¨ √Årea: {pdf_info['area_tema'][:60]}...\n"
                    
            if edital.get('pdf_baixado'):
                formatted += f"   üìÑ PDF: Baixado ‚úÖ\n"
            elif edital.get('url') and edital['url'].lower().endswith('.pdf'):
                formatted += f"   üìÑ PDF: Dispon√≠vel (n√£o baixado)\n"
            formatted += "\n"
            
        if len(editais) > 5:
            formatted += f"... e mais {len(editais) - 5} oportunidades\n"
            
        return formatted
        
    def cleanup(self):
        """Limpa recursos"""
        if self.driver:
            try:
                self.driver.quit()
                logger.info("‚úÖ Driver fechado com sucesso")
            except:
                pass

def send_email_unified(msg_text: str, subject: str = "üöÄ Scraping Unificado - Novas Oportunidades!"):
    """Envia email com os resultados unificados"""
    try:
        msg = MIMEMultipart()
        msg['Subject'] = subject
        msg['From'] = os.environ['EMAIL_USER']
        msg['To'] = os.environ.get('EMAIL_DESTINO', 'clevioferreira@gmail.com')
        
        # Corpo do email
        text_part = MIMEText(msg_text, 'plain', 'utf-8')
        msg.attach(text_part)
        
        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:
            server.login(os.environ['EMAIL_USER'], os.environ['EMAIL_PASS'])
            server.send_message(msg)
            
        logger.info(f"‚úÖ Email enviado com sucesso para {msg['To']}!")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao enviar email: {e}")
        return False

def main():
    """Fun√ß√£o principal"""
    logger.info("=" * 60)
    logger.info("üöÄ SCRAPER UNIFICADO - EDITAIS E CHAMADAS")
    logger.info("=" * 60)
    
    scraper = ScraperUnificado()
    
    try:
        # Configurar driver
        scraper.setup_driver()
        
        # Executar scraping
        scraper.run_scraping()
        
        # Salvar resultados
        scraper.save_results()
        
        # Preparar email
        if scraper.results['total_editais'] > 0:
            email_content = scraper._generate_summary()
            
            # Enviar email se as vari√°veis estiverem configuradas
            if 'EMAIL_USER' in os.environ and 'EMAIL_PASS' in os.environ:
                logger.info("üìß Enviando email...")
                if send_email_unified(email_content):
                    logger.info("‚úÖ Processo conclu√≠do com sucesso!")
                else:
                    logger.warning("‚ö†Ô∏è Scraping conclu√≠do, mas email n√£o foi enviado")
            else:
                logger.warning("‚ö†Ô∏è Vari√°veis de email n√£o configuradas")
                logger.info("üìß Conte√∫do que seria enviado:")
                logger.info("-" * 40)
                logger.info(email_content)
                logger.info("-" * 40)
        else:
            logger.warning("‚ö†Ô∏è Nenhuma oportunidade encontrada")
            
    except Exception as e:
        logger.error(f"‚ùå Erro cr√≠tico: {e}")
        
    finally:
        # Limpeza
        scraper.cleanup()

if __name__ == "__main__":
    main()
